{"Causal/Causal-Inference-for-The-Brave-and-True":{"slug":"Causal/Causal-Inference-for-The-Brave-and-True","filePath":"Causal/Causal Inference for The Brave and True.md","title":"Causal Inference for The Brave and True","links":[],"tags":["causal","todo"],"content":"01 - Introduction To Causality\nATE = Average Treatment Effect\nATT = Average Treatment Effect on the Treated\n\\begin{align}\nATE &amp;= E[Y_1 - Y_0] \\\\\nATT &amp;= E[Y_1 - Y_0 |T=1] \n\\end{align}\n\\begin{equation}\nE[Y \\mid T=1] - E[Y \\mid T=0] = \n\\underbrace{E[Y_1 - Y_0 \\mid T=1]}_{\\text{ATT}} + \n\\{ \\underbrace{E[Y_0 \\mid T=1] - E[Y_0 \\mid T=0]}_{\\text{BIAS}} \\}\n\\end{equation}\nThe bias is given by how the treated and control group differ before the treatment, in case neither of them has received the treatment.\n If E[Y_0|T=0]=E[Y_0|T=1], then, association IS CAUSATION!\nWhen there is bias:\n\nno bias:\n\n02 - Randomised Experiments\nRandomisation annihilates bias by making the potential outcomes independent of the treatment.\n(Y_0, Y_1) \\perp T\nSaying that the potential outcomes are independent of the treatment is saying that they would be, in expectation, the same in the treatment or the control group. … the only thing generating a difference between the outcome in the treated and in the control group.\nIn causal questions, we usually can argue in both ways: that X causes Y, or that it is a third variable Z that causes both X and Y, and hence the X and Y correlation is just spurious. For this reason, knowing the assignment mechanism leads to a much more convincing causal answer.\n03 - Stats Review: The Most Dangerous Equation\nMoivre’s equation talks about a fundamental fact about the reality of information and records in the form of data: it is always imprecise.\nstandard deviation, confidence intervals\nHypothesis testing, p-values\n04 - Graphical Causal Models\nconditional independence of the potential outcomes:  (Y_0, Y_1) \\perp T \\mid X\n\nA \\not\\perp C and A \\perp C \\mid B\n\nbackdoor path\nA \\not\\perp B and A \\perp B \\mid C\n\ncollider\nA \\perp B and A \\not\\perp B \\mid C\n A path is blocked if and only if:\n\nIt contains a non collider that has been conditioned on\nIt contains a collider that has not been conditioned on and has no descendants that have been conditioned on.\n\n\n\nD \\perp C. It contains a collider that it has not been conditioned on.\nD \\not\\perp C \\mid A. It contains a collider that it has been conditioned on.\nD \\not\\perp C \\mid G. It contains the descendent of a collider that has been conditioned on. You can see G as some kind of proxy for A here.\nA \\perp F. It contains a collider, B→E←F, that it has not been conditioned on.\nA \\not\\perp F \\mid E. It contains a collider, B→E←F, that it has been conditioned on.\nA \\perp F \\mid E, C. It contains a collider, B→E←F, that it has been conditioned on, but it contains a non collider that has been conditioned on. Conditioning on E opens the path, but conditioning on C closes it again.\n\nConfounding bias\nThe first significant cause of bias is confounding. It happens when the treatment and the outcome share a common cause.\n\nSelection Bias\nIf confounding bias happens when we don’t control for a common cause, selection bias is more related to effects. One word of caution here, economists tend to refer to all sorts of biases as selection bias.\nOften, selection bias arises when we control for more variables than we should. It might be the case that the treatment and the potential outcome are marginally independent but become dependent once we condition on a collider.\nAmong them, you control for investments. But investment is not a common cause of education and wage. Instead, it is a consequence of both. More educated people both earn more and invest more. Also, those who make more invest more. Since investment is a collider, by conditioning on it, you are opening a second path between the treatment and the outcome, which will make it harder to measure the direct effect. … But by doing so, you are also indirectly and inadvertently not allowing wages to change much.\n\nAs a side note, all of this we’ve discussed is true if we condition on any descendent of a common effect.\nA similar thing happens when we condition on a mediator of the treatment.\n\n05 - The Unreasonable Effectiveness of Linear Regression\ny_i=\\beta_0+\\kappa T_i + \\beta_1 X_{1i} + \\ldots + \\beta_k X_{ki} + u_i\nIt means that the coefficient of a multivariate regression is the bivariate coefficient of the same regressor after accounting for the effect of other variables in the model.\n… Even if we can’t use randomised controlled trials to keep other factors equal between treated and untreated, regression can do this by including those same factors in the model, even if the data is not random!\nOVB = Omitted Variable Bias\nTo put it more succinctly, we say that there is no OVB if all the confounding variables are accounted for in the model. We can also leverage our knowledge about causal graphs here.\n… This is to show that causal inference with non-random or observational data should always be taken with a grain of salt. We can never be sure that all confounders were accounted for.\n06 - Grouped and Dummy Regression\nThis phenomenon of having a region of low variance and another of high variance is called heteroskedasticity.\nweighting for the size of each group, if the variables are grouped\ninteraction terms for dummy variables\n07 - Beyond Confounders\nFortunately, regression can help us lower this variability. The trick is to use additional controls. If a variable is a good predictor of the outcome, it will explain away a lot of its variance.\nBut maybe controlling for it lowers the variance, right? Well, not true again. In order for a control to lower the variance, it has to be a good predictor of the outcome, not of the treatment, which is the case here.\nStill, we might want to control it right? It can’t hurt… Or can it?\nSurprisingly, it can hurt!\nSo the bottom line is that we should add controls that are both correlated with the treatment and the outcome (confounder), like the severity in the model above. We should also add controls that are good predictors of the outcome, even if they are not confounders, because they lower the variance of our estimates. However, we should NOT add controls that are just good predictors of the treatment, because they will increase the variance of our estimates.\n… However, both are part of the causal effect of the email, so we don’t want to hold them fixed. Instead, we could argue that email increases payments precisely because it boosts the agreement rate. If we fix those variables, we are removing some of the true effect from the email variable.\nThis sort of bias is so pervasive it has its own name. While confounding is the bias from failing to control for a common cause, selection bias is when we control for a common effect or a variable in between the path from cause to effect.\nConditional-on-Positives\nE[Y | T] = E[Y|Y &gt; 0, T] \\cdot P(Y &gt; 0|T)\n\nThe issue is in estimating the COP part. It will be biased even under random assignment. On an intuitive level, it’s not crazy to think that some units are zeros only because they didn’t get the treatment. The treatment would cause them to not be zeros.\nKnowing this, if we remove the zeros, we will keep the never zeros on both the treated and the control group. But we would remove from the control those that flip from zero to not zero with the treatment.\n\n\n\n08 - Instrumental Variables\nOne way to control for OVB is, well, adding the omitted variable into our model. However, that is not always possible, mostly because we simply don’t have data on the omitted variables.\nAnother way of saying this is that this instrument Zi is uncorrelated with Y0, but it is correlated with T. This is sometimes referred to as the exclusion restriction.\n\\begin{align}\nY_i &amp;= \\beta_0 + \\kappa T_i + v_i \\\\\nv_i &amp;= \\beta W_i + u_i\n\\end{align}\nwhere W is a confounder.\nBy definition, the instrumental variable Z should satisfy: Cov(Z, v) = 0\nCov(Z, Y) = Cov(Z, \\beta_0 + \\kappa T_i + v_i) = \\kappa Cov(Z, T) + Cov(Z, v) = \\kappa Cov(Z, T)\ntwo stage regression\n\\kappa = \\frac{Cov(Y_i, Z_i) / V(Z_i)}{Cov(T_i, Z_i) / V(Z_i)} = \\frac{\\text{Reduced Form}}{\\text{1st Stage}}\n\nCov(Z, T) \\neq 0. This is saying that we should have a strong 1st stage, or that the instrument indeed impacts the treatment variable.\nY \\perp Z | T. This is the exclusion restriction, stating that the instrument Z only affects the outcome Y through the treatment T.\n\nUnfortunately, we can’t verify the second IV condition. We can only argue in favor of it.\n\nHowever, it doesn’t matter how strong the second stage is if we have a weak first stage. A weak first stage means that the instrument has only a very small correlation with the treatment. Therefore, we can’t learn much about the treatment from the instrument.\n\nAs we can see in the plots above, estimates vary wildly when the correlation between T and Z is weak. This is because the SE also increases a lot when the correlation is low.\nAnother thing to notice is that 2SLS is biased! Even with high correlation, the parameter estimate still does not reach the true ATE of 2.0. Actually, 2.0 is not even in the 95% CI! 2SLS is only consistent, which means that it approaches the true parameter value if the sample size is big enough.\n\n\n2SLS is biased towards OLS. This means that if OLS has a negative/positive bias, 2SLS will also have it. The advantage of 2SLS is that it is at least consistent, where OLS is not, in the case of omitted variables. In the example above, our unobserved U impacts negatively the outcome but its positively correlated with the treatment, which will result in a negative bias. That is why we are seeing the ATE estimate below the true value (negative bias).\n\n\nThe bias will increase with the number of instruments we add. If we add too many instruments, 2SLS becomes more and more like OLS.\n\n\nBesides knowing how this bias behaves, a final piece of advice is to avoid some common mistakes when doing IV:\n\n\nDoing IV by hand. As we’ve seen, IV by hand will result in wrong standard errors, even if the parameter estimates are right. The SE won’t be completely off. Still, why do it if you can use software and get the right SE?\n\n\nUsing anything other than OLS on the 1st stage. Lots of Data Scientist encounter IV and think they can do better. For example, they see a dummy treatment and think about replacing the 1st stage by a logistic regression, after all, they are predicting a dummy variable, right?. The problem is that this is plain wrong. The consistency of IV relies on a property that only OLS can give, which is the orthogonality of the residuals, so anything different than OLS on the 1st stage will yield something biased. (OBS: there are some modern techniques that use Machine Learning for IV, but their results have been, at best, questionable).\n\n\n(when you do it by hand)\n\n09 - Non Compliance and LATE\nIt partitions the world into 4 kinds of subjects, depending on how they respond to the instrumental variable.\n\nCompliers\nNever Takers\nAlways Takers\nDefiers\n\nYou see, if we do this, we are actually answering a different question than the one we intended to. We would be finding the causal effect of treatment assignment, not of the treatment itself. … is the causal effect of treatment assignment an unbiased estimate of the ATE?\nAs it turns out, it is not. Because of non compliance, the outcome of those assigned to the treatment will be pushed in the direction of the outcome of those assigned to the control.\n\nLocal average treatment effect (LATE) makes it explicit the population for which we can estimate the causal effect.\nThis is the independence Assumption. This says that the instrument is as good as randomly assigned. In other words, Z, the instrument, is not correlated with the potential treatments, which is the same as saying that people in different instrument groups are comparable. → does not target any particular “type” of person, e.g. always-takers or compliers\nZ = assignment of treatment\npotential treatment T_{0i}, T_{1i} \\neq actual treatment T\n\nCompliers means that Ti1&gt;Ti0\nNever Takers Ti1=Ti0=0\nAlways Takers Ti1=Ti0=1\n\nThe conclusion of this is that IV says nothing about the effect on never takers, always takers or defiers, because the treatment is unchanged for them! IV only finds the treatment effect for the compliers.\n… This shows that the result with 2SLS is much lower than the one we got with OLS: 3.29 against 27.60. This makes sense, since the causal effect estimated with OLS is positively biased. We also need to remember about LATE. 3.29 is the average causal effect on compliers. Unfortunately, we can’t say anything about those never takers.\n10 - Matching\nThis estimate is done by 1) partitioning the data into confounder cells, in this case, men and women, 2) estimating the effect on each cell and 3) combining the estimate with a weighted average, where the weight is the sample size of the cell or covariate group. … This is called a non-parametric estimate, since it places no assumption on how the data was generated.\nIf we control for sex using regression, we will add the assumption of linearity. … Instead, regression uses weights that are proportional to the variance of the treatment in that group.\nThe Subclassification Estimator\nATE = \\int\\left(E[Y|X,T=1] - E[Y|X,T=0]\\right)dP(x)\nMatching Estimator\nSince some sort of confounder X makes it so that treated and untreated are not initially comparable, I can make them so by matching each treated unit with a similar untreated unit.\nwe have to define some measurement of proximity to compare how units are close to each other. One common metric for this is the euclidean norm ||Xi−Xj||. This difference, however, is not invariant to the scale of the features. This means that features like age, that take values on the tenths, will be much less important when computing this norm compared to features like income, which take the order of hundreds. For this reason, before applying the norm, we need to scale the features so that they are on roughly the same scale.\n\\begin{align}\n\\hat{ATE} &amp;= \\frac{1}{N}\\sum_{i=1}^N (2T_i - 1)(Y_i - Y_{jm}(i)) \\\\\n\\hat{ATET} &amp;= \\frac{1}{N_1} \\sum \\left(Y_i - Y_{j(i)} \\right)\n\\end{align}\nwhere Y_{jm}(i) is the sample from the other treatment group which is most similar to Y_i.\nBias arises when the matching discrepancies are huge. Fortunately, we know how to correct it. Each observation contributes (μ0(Xi)−μ0(Xj(i))) to the bias so all we need to do is subtract this quantity from each matching comparison in our estimator. To do so, we can replace μ0(Xj(i)) with some sort of estimate of this quantity μ0^(Xj(i)), which can be obtained with models like linear regression.\n\\hat{ATET} = \\frac{1}{N_1} \\sum \\left((Y_i - Y_{j(i)}) - (\\hat{\\mu_0}(X_i) - \\hat{\\mu_0(X_{j(i)})}) \\right)\n11 - Propensity Score\nPropensity score comes from the realisation that you don’t need to directly control for confounders X to achieve conditional independence (Y_1,Y_0) \\perp T \\mid X. Instead, it is sufficient to control for a balancing score E[T|X]. This balancing score is often the conditional probability of the treatment, P(T|X), also called the propensity score e(x).\n(Y_1, Y_0) \\perp T \\mid e(x)\n\nIf I know what e(x) is, X alone tells me nothing more that can help me learn what T would be. Which means that controlling for e(x) acts the same way as controlling for X directly.\n\nNotice that this estimator requires that e(x) and 1−e(x) are larger than zero. In words, this means that everyone needs to have at least some chance of receiving the treatment and of not receiving it. Another way of stating this is that the treated and untreated distributions need to overlap. This is the positivity assumption of causal inference.\nWe can also use the propensity score to find evidence of confounding. If a segmentation of the population has a higher propensity score than another, it means that something which is not random is causing the treatment. If that same thing is also causing the outcome, we have confounding.\n\n\nTo compute the standard error for the IPTW estimator, we can use the formula of the variance of a weighted average.\nσw2=∑i=1nwi(yi−μ^)2∑i=1nwi\nHowever, we can only use this if we have the true propensity score. If we are using the estimated version of it, P^(x), we need to account for the errors in this estimation process. The easiest way of doing this is by bootstrapping the whole procedure. This is achieved by sampling with replacement from the original data and computing the ATE like we did above. We then repeat this many times to get the distribution of the ATE estimate.\n… Propensity score doesn’t need to predict the treatment very well. It just needs to include all the confounding variables. If we include variables that are very good in predicting the treatment but have no bearing on the outcome this will actually increase the variance of the propensity score estimator.\nTo see this, consider the following example (adapted from Hernán’s Book). You have 2 schools, one of them apply the growth mindset seminar to 99% of its students and the other to 1%. Suppose that the schools have no impact on the treatment effect (except through the treatment), so it’s not necessary to control for it. If you add the school variable to the propensity score model, it’s going to have a very high predictive power. However, by chance, we could end up with a sample where everyone in school A got the treatment, leading to a propensity score of 1 for that school, which would lead to an infinite variance.\n… We actually need to construct the prediction in a way that controls for confounding, not in a way to predict the treatment.\nPropensity Score Matching\nIf we control for the propensity score, we now estimate a ATE of 0.39, which is lower than the 0.47 we got previously with a regression model without controlling for the propensity score. We can also use matching on the propensity score. This time, instead of trying to find matches that are similar in all the X features, we can find matches that just have the same propensity score.\n(matching on propensity scores) … One final word of caution here is that the above standard errors are wrong, as they don’t account for the uncertainty in the estimation of the propensity score. Unfortunately, bootstrap doesn’t work with matching.\n12 - Doubly Robust Estimation\n\nFirst, it is called doubly robust because it only requires one of the models, P^(x) or μ^(x), to be correctly specified.\nif \\hat{μ_1}(x) is correct, then E[T_i(Y_i−\\hat{μ_1}(X_i))]=0.\n\nNow, assume that the propensity score \\hat{P}(X_i) is correctly specified. In this case, E[T_i−\\hat{P}(X_i)]=0, which wipes out the part dependent on \\hat{μ_1}(X_i).\n… Its magic happens because in causal inference, there are two ways to remove bias from our causal estimates: you either model the treatment mechanism or the outcome mechanism. If either of these models are correct, you are good to go.\nOne caveat is that, in practice, it’s very hard to model precisely either of those. More often, what ends up happening is that neither the propensity score nor the outcome model are 100% correct. They are both wrong, but in different ways. When this happens, it is not exactly settled 1 2 3 if it’s better to use a single model or doubly robust estimation. As for me, I still like using them because at least it gives me two possibilities of being correct.\n13 - Difference-in-Differences\n… In all these cases, you have a period before and after the intervention and you wish to untangle the impact of the intervention from a general trend.\n\nOne obvious problem with Diff-in-Diff is failure to satisfy the parallel trend assumption.\n\n14 - Panel Data and Fixed Effects\n… But what would happen if we had more periods? Or more groups? Turns out this setup is so common and powerful for causal inference that it gets its own name: panel data.\nOne way to see the parallel (or common) trends assumptions is as an independence assumption. If we recall from very early chapters, the independence assumption requires that the treatment assignment is independent from the potential outcomes:\nY_d \\perp D \\mid X\nIf the traditional independence assumption states that the treatment assignment can’t be related to the levels of potential outcomes, the parallel trends states that the treatment assignment can’t be related to the growth in potential outcomes over time.\n\\left(Y_d(t) - Y_d(t - 1)\\right) \\perp D\n… All we need to do is create dummy variables indicating that person and add that to a linear model. This is what we mean when we say we can control for the person itself: we are adding a variable (dummy in this case) that denotes that particular person. When estimating the effect of marriage on income with this person dummy in our model, regression finds the effect of marriage while keeping the person variable fixed. Adding this unit dummy is what we call a fixed effect model.\nNow, remember how I’ve said that using panel data with a fixed effect model is as simple as adding a dummy for the entities. It’s true, but in practice, we don’t actually do it. Imagine a dataset where we have 1 million customers. If we add one dummy for each of them, we would end up with 1 million columns, which is probably not a good idea. Instead, we use the trick of partitioning the linear regression into 2 separate models. We’ve seen this before, but now is a good time to recap it. Suppose you have a linear regression model with a set of features X1 and another set of features X2.\n\n… First, we use the dummies to predict the outcome and the feature. These are steps 1 and 2 above.\n\nNotice that fixed effect is fitting one regression line per city. Also notice that the lines are parallel. The slope of the line is the effect of marketing costs on in-app purchase. So the fixed effect is assuming that the causal effect is constants across all entities, which are cities in this case.\n\nJust like we did a fixed effect for the individual level, we could design a fixed effect for the time level. If adding a dummy for each individual controls for fixed individual characteristics, adding a time dummy would control for variables that are fixed for each time period, but that might change across time.\nThere are situations where even panel data won’t help you. … The most obvious one is when you have confounders that are changing in time. Fixed effects can only eliminate bias from attributes that are constant for each individual.\nAnother less obvious case when fixed effect fails is when you have reversed causality. For instance, let’s say that it isn’t marriage that causes you to earn more. Is earning more that increases your chances of getting married.\n15 - Synthetic Control\n… However, note that the sample size here is 4, which is also the number of parameters in our Diff-in-Diff models. In this case, the standard error is not well defined, so what should we do? Another problem is that Florianopolis might not be as similar to Porto Alegre as we would want to.\nTo work around this, we will use what is known as “the most important innovation in the policy evaluation literature in the last few years”, Synthetic Controls. It is based on a simple, yet powerful idea. We don’t need to find any single unit in the untreated that is very similar to the treated. Instead, we can forge our own as a combination of multiple untreated units …\nFor each unit j and each time t, we observe the outcome Yjt. For each unit j and period t, define YjtN as the potential outcome without intervention and YjtI, the potential outcome with intervention. Then, the effect for the treated unit j=1 at time t, for t&gt;T0 is defined as\n\\tau_{1t} = Y_{1t}^I - Y_{1t}^N\nSince unit j=1 is the treated one, Y1tI is factual but Y1tN is not. The challenge then becomes how do we estimate Y1tN. Notice how the treatment effect is defined for each period, which means it can change in time. It doesn’t need to be instantaneous. It can accumulate or dissipate. To put it in a picture, the problem of estimating the treatment effect boils down to the problem of estimating what would have happened to the outcome of unit j=1 if it had not been treated.\n\nTo estimate the treatment effect with synthetic control, we will try to build a “fake unit” that resembles the treated unit before the intervention period. Then, we will see how this “fake unit” behaves after the intervention. The difference between the synthetic control and the unit that it mimics is the treatment effect.\n… This is the case where, even if T is large, N is also large, which gives too much flexibility to our linear regression model. If you are familiar with regularized models, know that you could use Ridge or Lasso regression to fix this. Here, we will look at another more traditional way to avoid overfitting.\nOne way to play safer is to constrain our synthetic control to only do interpolation. To do so, we will restrict the weights to be positive and sum up to one.\n\nNotice two things here. First, interpolation won’t be able to create a perfect match of the treated unit in this case. This is because the treated is the unit with the smallest number of sales and the highest price. Convex combinations can only replicate exactly features that are in between the control units.\nAnother thing to notice is that interpolation is sparse. We will project the treated unit on a wall of the convex hull and this wall is defined only by a few units.\n\nHere, we will use the idea of Fisher’s Exact Test. Its intuition is very simple. We permute the treated and control exhaustively. Since we only have one treated unit, this would mean that, for each unit, we pretend it is the treated while the others are the control.\n… So what this does is it pretends that the treatment actually happened for another state, not California, and see what would have been the estimated effect for this treatment that didn’t happen. Then, we see if the treatment in California is sufficiently larger when compared to the other fake treatment. The idea is that for states that weren’t actually treated, once we pretend they were, we won’t be able to find any significant treatment effect.\n\nif we want to test the one sided hypothesis that the effect in California is below zero, we can estimate the P-value as the proportion of times the effect in California is bigger than all the estimated effects.\n16 - Regression Discontinuity Design\n\nThis is, in its own way, a sort of Local Average Treatment Effect (LATE), since we can only know it at the threshold. In this setting, we can think of RDD as a local randomized trial. For those at the threshold, the treatment could have gone either way and, by chance, some people fell below the threshold, and some people fell above.\ny_i = \\beta_0 + \\beta_1r_i + \\beta_2 1\\{r_i &gt; c\\} + \\beta_3 1\\{r+i &gt; c\\}r_i\n\n… What can happen is that regression might focus too much on fitting the other data points at the cost of a poor fit at the threshold. If this happens, we might get the wrong measure of the treatment effect.\nOne way to solve this is to give higher weights for the points that are closer to the threshold. There are many ways to do this, but a popular one is to reweight the samples with the triangular kernel\nK(R, c, h) = 1\\{\\vert R - c \\vert \\leq h \\} * \\left(1 - \\frac{\\vert R - c \\vert}{h}\\right)\n… Here, the regression discontinuity is fuzzy, rather than sharp. Notice how the probability of getting the diploma doesn’t jump from zero to one at the threshold. But it does jump from something like 50% to 90%.\n\nJust like when we have the potential outcome, we have the potential treatment status in this situation. T1 is the treatment everyone would have received had they been above the threshold. T0 is the treatment everyone would have received had they been below the threshold. As you’ve might have noticed, we can think of the threshold as an Instrumental Variable. Just as in IV, if we naively estimate the treatment effect, it will be biased towards zero.\n\n\n… Students are not manipulating where they fall on the threshold. Just for illustrative purposes, the second plot shows what bunching would look like if students could manipulate where they fall on the threshold."},"RecSys/A-Comprehensive-Review-of-Recommender-Systems---Transitioning-from-Theory-to-Practice":{"slug":"RecSys/A-Comprehensive-Review-of-Recommender-Systems---Transitioning-from-Theory-to-Practice","filePath":"RecSys/A Comprehensive Review of Recommender Systems - Transitioning from Theory to Practice.md","title":"A Comprehensive Review of Recommender Systems - Transitioning from Theory to Practice","links":["RecSys/Dynamic-Graph-Neural-Networks-for-Sequential-Recommendation","RecSys/Frequency-Enhanced-Hybrid-Attention-Network-for-Sequential-Recommendation","RecSys/Knowledge-Prompt-tuning-for-Sequential-Recommendation","RecSys/Bayesian-Knowledge-driven-Critiquing-with-Indirect-Evidence","RecSys/DiffKG---Knowledge-Graph-Diffusion-Model-for-Recommendation","RecSys/Towards-Hierarchical-Policy-Learning-for-Conversational-Recommendation-with-Hypergraph-based-Reinforcement-Learning","RecSys/RecMind---Large-Language-Model-Powered-Agent-For-Recommendation","RecSys/Adaptive-Multi-Modalities-Fusion-in-Sequential-Recommendation-Systems","RecSys/PromptMM---Multi-Modal-Knowledge-Distillation-for-Recommendation-with-Prompt-Tuning","RecSys/Explainable-Fairness-in-Recommendation","RecSys/Bias-Reduction-News-Recommendation-System","RecSys/Fairness-among-New-Items-in-Cold-Start-Recommender-Systems","RecSys/Monolith---Real-Time-Recommendation-System-With-Collisionless-Embedding-Table","RecSys/Consistent-Collaborative-Filtering-via-Tensor-Decomposition"],"tags":["survey","recsys","todo"],"content":"Abstract\nRecommender Systems (RS) play an integral role in enhancing user experiences by providing personalized item suggestions. This survey reviews the progress in RS inclusively from 2017 to 2024, effectively connecting theoretical advances with practical applications. We explore the development from traditional RS techniques like content-based and collaborative filtering to advanced methods involving deep learning, graph-based models, reinforcement learning, and large language models. We also discuss specialized systems such as context-aware, review-based, and fairness-aware RS. The primary goal of this survey is to bridge theory with practice. It addresses challenges across various sectors, including e-commerce, healthcare, and finance, emphasizing the need for scalable, real-time, and trustworthy solutions. Through this survey, we promote stronger partnerships between academic research and industry practices. The insights offered by this survey aim to guide industry professionals in optimizing RS deployment and to inspire future research directions, especially in addressing emerging technological and societal trends2 . The survey resources are available in the public GitHub repository github.com/VectorInstitute/Recommender-Systems-Survey.\nIntroduction\nMain Contributions\n\nThis survey provides a comprehensive review of RS, tracing their development from theoretical foundations to practical applications between 2017 and 2023. It is the first survey to specifically highlight the translation of theoretical advancements into practical solutions for industry challenges.\nEach type of RS is thoroughly examined, including data input methods, associated challenges, relevant datasets, evaluation metrics, model accuracy, and practical applications, as presented in tables. The survey aims to offer industry professionals a set of guidelines to facilitate the deployment of these systems in real-world settings.\nWe discuss the specific challenges faced by RS in various sectors, such as e-commerce, healthcare, finance, and others. The survey emphasizes the need for scalable, real-time, and privacy-focused solutions, demonstrating how theoretical insights can address these industry-specific demands.\n\n\nTraditional RS methods can be categorized into collaborative filtering, content-based filtering, and hybrid approaches.\nCollaborative filtering (CF) [70] is based on the idea that users with similar preferences will likely have similar tastes in the future. CF recommends items by finding a neighborhood of similar users or items. CF can recommend items without needing much content analysis, however, it normally faces challenges like cold starts, scalability, and sparsity.\n\nContent-based filtering (CBF) [72] recommends items based on a user past preferences and item characteristics, using techniques like Term Frequency- Inverse Document Frequency (TF-IDF), cosine similarity, and neural networks for item representation. However, it may struggle with recommending new or unseen items.\n\nHybrid RS [36] combine the strengths of both approaches, offering more accurate and personalized recommendations by integrating diverse methodologies.\n\nModeling Techniques\nGraph-based Recommender Systems\nGNNs can effectively address various practical challenges by modeling complex relationships in data.\n\n\nDynamic Graph Neural Networks for Sequential Recommendation\n\nSequential and Session-based Recommender Systems\nSequential recommendation is commonly viewed as a next-item or next-basket prediction challenge [37]. Both the sequential and session-based RS leverage user action sequences to anticipate users’ future preferences [95]. Specifically, sequential RS consider the interaction histories of the users to predict future behaviour or users’ preferences. In contrast, session-based RS, detailed in survey [38], focus on short-term user activity for real-time recommendations. These approaches collectively enhance personalization and relevance across diverse platforms.\n\nFrequency Enhanced Hybrid Attention Network for Sequential Recommendation\nKnowledge Prompt-tuning for Sequential Recommendation\n\nKnowledge-based Recommender Systems\nA KG is a directed graph G = (V, E), where V and E represent entities and relations between them, respectively, with E ⊆ V × V . It includes entity type function Φ : V → A and relation type function Ψ : E → R, mapping entities to types A and relations to types R. KGs are depicted as sets of triples ⟨eh, r, et⟩, signifying a relation r from eh to et.\nEmbedding-based approaches focus on learning and applying embeddings to represent KG entities (nodes) and relations (edges), enhancing user and item representations. They typically start with initial embedding generation using models like TransE [309], TransD [198], and node2vec [310], followed by their application in RS through attention mechanisms in KSR [229] or generative models like BEM [176] and KTGAN [311].\nJoint Learning Methods optimize both KG embeddings and recommendation components simultaneously using a unified loss function. Examples include CKE [112], which integrates auto-encoders for item representations, and SHINE [312], which acquires user embeddings from heterogeneous graphs. Multi-Task Methods such as KTUP [221] and MKR [206] address KG-enhanced recommendation and KG completion concurrently, improving both entity/relation representations and recommendations.\nPropagation-based approaches influence embeddings through multi-hop neighbor interactions within the KG. Item KG-based methods like Ripplenet [215] aggregate item-related embeddings to derive user interests, whereas User-Item KG-based methods such as KGAT [197] and Intentgc [195] refine both user and item embeddings by propagating embeddings across a user-item graph, enhancing recommendation accuracy.\nOverall, these systems enable more contextually aware, personalized, and efficient recommendation systems, significantly improving user experience across these sectors.\n\nBayesian Knowledge-driven Critiquing with Indirect Evidence\nDiffKG - Knowledge Graph Diffusion Model for Recommendation\n\nReinforcement Learning-based Recommender Systems\nBy employing techniques such as deep Q-networks and policy gradient methods, RL-based recommender systems continuously refine their decision-making processes, leading to improved long-term user engagement and satisfaction.\n\nTowards Hierarchical Policy Learning for Conversational Recommendation with Hypergraph-based Reinforcement Learning\n\nLarge Language Model based Recommender Systems\nThe integration of BERT-like models into RS has led to significant advancements. Initial applications like BERT4REC [228] utilized deep bidirectional self-attention for modeling user behavior sequences, while further developments employed BERT for tasks ranging from conversational RS [78] to CTR prediction [339]. Enhancements in BERT-based models have addressed specific RS challenges, such as item alignment in dialogues [340] and user representation through models like U-BERT [341] and UserBERT [342]. Further innovations include BERT-based re-ranking [343] and addressing data sparsity in group recommendations [344].\nPrompt-based and in-context learning (ICL) approaches have leveraged the adaptability of LLMs, employing personalized prompts and natural language processing to enhance recommendation relevance and user interaction without extensive retraining [345, 293].\nLLMs have advanced RS by addressing key challenges such as the cold-start problem, enhancing personalization, and improving accuracy.\n\nRecMind - Large Language Model Powered Agent For Recommendation\n\nMultimodal Recommender Systems\nThe evolution of multi-modal RS began with the introduction of Visual Bayesian Personalized Ranking (VBPR) [349], which enhances personalized ranking by integrating visual features from product images. The results showed improved accuracy and addressing cold-start issues. Attentive Collaborative Filtering (ACF) [350] introduced a novel attention mechanism to better handle item- and component-level feedback in multimedia recommendations.\nCollaborative Cross Networks (CoNet) [351] utilizes deep transfer learning. Multi-Modality enriched Sequential Recommendation (MMSR) [249], a graph-based model, adaptively fuses multi-modal information to dynamically prioritize modalities based on their sequential relationships.\n\nAdaptive Multi-Modalities Fusion in Sequential Recommendation Systems\nPromptMM - Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning\n\nSpecialized Recommender Systems\nContext-aware Recommender Systems\nContext-aware recommender systems (CARS) are advanced RS that enhance the personalization of content by incorporating contextual information into the recommendation process [33]. Unlike traditional RS that primarily rely on user-item interactions, CARS consider additional dimensions such as time, location, social settings, and user behavior patterns to deliver more relevant and timely suggestions [352].\nAmong these, factorization machines (FM) [353] are prominent for their ability to capture interactions between variables within large datasets. Field-Aware Factorization Machines (FFMs) [354] are specifically optimized for CTR prediction, showing the versatility and depth of models developed for enhancing CARS’ performance.\nReview-based Recommender Systems\nA review-based RS uses textual reviews and ratings from users to generate personalized recommendations for products or services [358, 45]. The review-based RS have evolved by improving through various models. Initially, models like Hidden Factors as Topics (HFT) [359] aligned topics from reviews with latent dimensions from ratings. Successive approaches, such as Rating-Boosted Latent Topics (RBLT) [360], Topic Initialized Latent Factor Model (TIM) [307], and deep learning models like Convolutional Matrix Factorization (ConvMF) [361] and Deep Cooperative Neural Networks (DeepCoNN) [87], utilized neural networks to better handle sparse data and extract nuanced features from reviews.\nAspect-based Recommender Systems\nAspect-based RS extract and analyze specific product attributes from reviews, providing tailored recommendations to the users based on item aspects [376]. This approach to RS differs with review-based RS, which assess overall user sentiment and preferences from review content.\nAspect-based RS effectively address several practical challenges by focusing on specific product attributes extracted from user reviews. These systems enhance personalization by tailoring recommendations based on individual user preferences and item characteristics.\nExplainable and Trustworthy Recommender Systems\nAdvancements in explainable and trustworthy RS have evolved, starting with phrase-level analysis of user reviews to enhance recommendation explainability by identifying critical item aspects [389]. Subsequent models like Tripartite Graph Ranking (TriRank) have improved top-N recommendations by extracting aspects from reviews and creating a user-item-aspect ternary relation [390]. Concurrently, models such as the Tree-Enhanced Embedding Model (TEM) merge embedding-based and tree-based methods with an attention network to ensure transparency, utilizing rich side information and explicit decision rules [391]. This integration extends to combining CF with structured knowledge bases and unstructured data like textual reviews for personalized and understandable recommendations [392]. Additionally, techniques like RL have been applied to generate flexible, high-quality explanations across recommendation models [251].\nRecent efforts like the Counterfactual Explainable Fairness (CEF) framework focus on identifying and mitigating fairness issues in RS [397].\n\nExplainable Fairness in Recommendation\n\nFairness, Accountability, Transparency, and Ethics (FATE) in Recommender Systems\nFairness in RS, as outlined in [401], refers to the ethical principle and requirement that recommender algorithms allocate resource (information, opportunities, or exposure) in a manner that is equitable and just across different users and items.\nPre-processing Fairness Methods Pre-processing efforts for fairness in RS involve adjusting training data, altering proportions of protected groups (like gender, race, age) through resampling [174] or adding synthetic data [402]. These methods aim to mitigate biases in input data before model training, they struggle to entirely eliminate biases that appear during training or inference.\nIn-processing Fairness Methods In-processing fairness methods in RS primarily utilize ranking approaches and advanced techniques to incorporate fairness directly into model training, yielding more immediate improvements by modifying elements closely tied to the final output. Regularization techniques play a crucial role by embedding fairness constraints or penalties into the objective function to balance accuracy with fairness\nAdversarial learning further enhances fairness by learning representations that maintain independence from sensitive attributes or ensure equitable distribution across groups\nPost-Processing Fairness Methods Post-processing methods involve adjusting the initial output of a recommendation model to satisfy certain fairness criteria before presenting the final recommendations to users.\n\nBias Reduction News Recommendation System\nFairness among New Items in Cold Start Recommender Systems\n\nApplications\nNumerous platforms have leveraged advanced RS technologies to enhance user engagement and content personalization. YouTube employs deep neural networks to refine its recommendation process, focusing on optimal ranking and selection of videos [488]. Google Play utilizes both linear models and neural networks within its Wide &amp; Deep Learning framework to achieve a balance between memorization and generalization [97]. LinkedIn enhances job and content recommendation using real-time processing and scoring mechanisms, integrating CF and deep learning to match job seekers with suitable opportunities [132, 489]. Twitter customizes its content recommendations, like tweets and follower suggestions, based on user behavior and preferences [490].\nByteDance has introduced innovative models for TikTok to quickly adapt recommendations to user interactions, employing unique retrieval models and scalable systems like Monolith, which uses collisionless embedding tables for efficient memory usage [491, 492]. Apple has developed the Sliced Anti-symmetric Decomposition (SAD) model to enhance collaborative filtering, allowing more nuanced user-item interactions, and explores controlled music production using diffusion models [493, 494]. DeepMind’s generative models improve RS by decoding Semantic IDs from user interactions, enhancing item retrieval and system performance [495].\n\nMonolith - Real Time Recommendation System With Collisionless Embedding Table\nConsistent Collaborative Filtering via Tensor Decomposition\n"},"RecSys/Adaptive-Multi-Modalities-Fusion-in-Sequential-Recommendation-Systems":{"slug":"RecSys/Adaptive-Multi-Modalities-Fusion-in-Sequential-Recommendation-Systems","filePath":"RecSys/Adaptive Multi-Modalities Fusion in Sequential Recommendation Systems.md","title":"Adaptive Multi-Modalities Fusion in Sequential Recommendation Systems","links":["RecSys/Learning-Vector-Quantized-Item-Representation-for-Transferable-Sequential-Recommenders"],"tags":["recsys"],"content":"Abstract\nIn sequential recommendation, multi-modal information (e.g., text or image) can provide a more comprehensive view of an item’s profile. The optimal stage (early or late) to fuse modality features into item representations is still debated. We propose a graph-based approach (named MMSR) to fuse modality features in an adaptive order, enabling each modality to prioritize either its inherent sequential nature or its interplay with other modalities. MMSR represents each user’s history as a graph, where the modality features of each item in a user’s history sequence are denoted by cross-linked nodes. The edges between homogeneous nodes represent intra-modality sequential relationships, and the ones between heterogeneous nodes represent inter-modality interdependence relationships. During graph propagation, MMSR incorporates dual attention, differentiating homogeneous and heterogeneous neighbors. To adaptively assign nodes with distinct fusion orders, MMSR allows each node’s representation to be asynchronously updated through an update gate. In scenarios where modalities exhibit stronger sequential relationships, the update gate prioritizes updates among homogeneous nodes. Conversely, when the interdependent relationships between modalities are more pronounced, the update gate prioritizes updates among heterogeneous nodes. Consequently, MMSR establishes a fusion order that spans a spectrum from early to late modality fusion. In experiments across six datasets, MMSR consistently outperforms state-of-the-art models, and our graph propagation methods surpass other graph neural networks. Additionally, MMSR naturally manages missing modalities.\nIntroduction\nModality information, such as images or text, has been extensively studied in collaborative recommendation [3, 43, 46, 57], but its potential in sequential recommendation (SR) remains largely unexplored. In collaborative recommendation, modalities are represented as high-dimensional feature vectors, which are captured through pretrained models like BERT [9] for texts and ResNet [13] for images.\nHowever, incorporating multiple modalities into SR poses two key challenges: (1) Identifying sequential patterns within each modality, as they may exhibit distinct patterns; (2) Capturing the complex interplay between modalities that can influence users’ sequential behavior.\nIn Sequential Recommendation, existing approaches for merging different channels of features include early [19, 25, 39] and late fusion [54], which determine whether merging occurs before or after sequential modeling. … early fusion is less sensitive to the interactions between intra-channel features, while late fusion is less sensitive to the interactions among different channels of features.\n\n… We found late fusion models are more sensitive to the disordered version (resulting in a significant performance drop). In contrast, early fusion is less sensitive to sequential patterns within each channel. Under mismatched conditions, this reversed, with early fusion experiencing a larger performance drop.\nThese findings reveal that fusion order is crucial. While holistic fusion methods like Trans2D [40] suggest features can be fused without a strict order, they do not address the heterogeneity of feature channels or consider fusion order impact. … Our MMSR framework comprises three stages: representation, where item features in each channel are represented as nodes; fusion, which aggregates features from different channels using graph techniques; and prediction, which generates the final representations.\nWe represent each user’s behavior history with a graph, where the modality features of items are nodes. We consider three feature channels: item identifier, visual, and textual modalities.\nFirstly, in graph construction, treating each modality (such as images) as an individual node will overlook their semantic relatedness.\n→ to construct graphs, we adopt a similar approach [51] to create compositional embeddings that represent nodes as compositions of smaller groups. Specifically, we cluster modality features and select the identifiers of the cluster centers as modality codes, which are then treated as new nodes in the graph.\nSecondly, graph nodes and relations are typed.\n→ for graph aggregation, we employ a dual attention function that distinguishes between homogeneous and heterogeneous nodes’ correlations.\nThirdly, naïve graph updating is synchronous for all nodes, unable to support fusion order.\n→ for graph updating, in MMSR, each node adaptively chooses the order of fusion through an update gate.\nWe summarise our contributions as follows: (i) We spotlight challenges in modality fusion for sequential recommendation, and propose a versatile solution — our MMSR framework. It accommodates both early and late fusion across modalities. (ii) We offer a graph-centric holistic fusion method as the engine in MMSR, enabling the adaptive selection of fusion order for each feature node. (iii) We conduct comprehensive experiments on six datasets, which show significant gains in both accuracy and robustness.\nPreliminaries\nIn our problem, the core task is sequential recommendation: Given a user 𝑢’s historical interaction data H𝑢, the aim is to find a function 𝑓 : H𝑢 → 𝑣 that predicts the next item 𝑣 that the user is most likely to consume.\n\nApproach\nFor each user𝑢, we represent his/her history as a graph — a Modality-enriched Sequence Graph (MSGraph), G𝑢 = (N𝑢, R, E𝑢). Note that each user’s graph N𝑢 and E𝑢 can differ.\nNodes and their initialization. Each MSGraph should consist of 𝑚 × 3 nodes (where 𝑚 is the sequence length), forming the node set N. N encompasses the three types of nodes, representing three distinct features of channels: {𝑣1, …, 𝑣𝑚}, {𝑎1, …, 𝑎𝑚}, and {𝑏1, …, 𝑏𝑚}. Their representations are associated with the first row (item ID feature), second row (image feature), and third row (text feature) of matrix representation tensor E, respectively.\nNode transformation and compositions. According to Hou et al. [20], closely binding text encodings with item representations can be detrimental. Thus, instead of using each modality as an individual node, we introduce “modality codes” [20, 36] as alternative nodes.\n\nLearning Vector-Quantized Item Representation for Transferable Sequential Recommenders\n\nEdges and Relation Types. . In the MSGraphs, we specify the edges as relations E between nodes, including homogeneous relations Eℎ𝑜𝑚𝑜 and heterogeneous relations Eℎ𝑒𝑡𝑒. … Additionally, in both types of relations, we introduce self-loop relations for each node to preserve its original information.\nHeterogeneity-aware. To aggregate homogeneous and heterogeneous neighbor nodes, we employ a divide-and-conquer strategy. … they represent neighbors that differ in type from the central node.\nFor attention regarding homogeneous nodes, their shared space allows direct comparison. We employ content-based attention for this.\n\nFor the attention calculation with respect to heterogeneous nodes, as they are located in different spaces, so we employ type-specific transformation matrices (𝑡𝑗 ≠ 𝑡𝑖 ) to bring them into a common space for comparison.\n\nAsynchronous updating. Synchronous updating overlooks the effect of the fusion order. Therefore, we propose an asynchronous updating strategy with two defined updating orders.\n\nNon-invasive fusion. Drawing inspiration from NOVA [28], we employed a non-invasive technique to limit interference among different node types during feature updates. For example, although the image features are fused with the item node in Phase 1, they do not actually update it but only use the updated representation for calculating the attention scores in Phase 2.\nExperiment\nIn line with previous studies [15, 53], we utilized the Amazon review dataset [14] for evaluation. This dataset provides both product descriptions and images, with varying sizes across product categories. To showcase our approach’s versatility, we selected six datasets from diverse categories: Beauty, Clothing, Sport, Toys, Kitchen, and Phone.\n\n\n"},"RecSys/Bayesian-Knowledge-driven-Critiquing-with-Indirect-Evidence":{"slug":"RecSys/Bayesian-Knowledge-driven-Critiquing-with-Indirect-Evidence","filePath":"RecSys/Bayesian Knowledge-driven Critiquing with Indirect Evidence.md","title":"Bayesian Knowledge-driven Critiquing with Indirect Evidence","links":["RecSys/SimplE-Embedding-for-Link-Prediction-in-Knowledge-Graphs"],"tags":["recsys"],"content":"Abstract\nConversational recommender systems (CRS) enhance the expressivity and personalization of recommendations through multiple turns of user-system interaction. Critiquing is a well-known paradigm for CRS that allows users to iteratively refine recommendations by providing feedback about attributes of recommended items. While existing critiquing methodologies utilize direct attributes of items to address user requests such as ‘I prefer Western movies’, the opportunity of incorporating richer contextual and side information about items stored in Knowledge Graphs (KG) into the critiquing paradigm has been overlooked. Employing this substantial knowledge together with a well-established reasoning methodology paves the way for critique-based recommenders to allow for complex knowledge-based feedback (e.g., ‘I like movies featuring war side effects on veterans’) which may arise in natural user-system conversations. In this work, we aim to increase the flexibility of critique-based recommendation by integrating KGs and propose a novel Bayesian inference framework that enables reasoning with relational knowledge-based feedback. We study and formulate the framework considering a Gaussian likelihood and evaluate it on two well-known recommendation datasets with KGs. Our evaluations demonstrate the effectiveness of our framework in leveraging indirect KG-based feedback (i.e., preferred relational properties of items rather than preferred items themselves), often improving personalized recommendations over a one-shot recommender by more than 15%. This work enables a new paradigm for using rich knowledge content and reasoning over indirect evidence as a mechanism for critiquing interactions with CRS.\nIntroduction\n\nIn this work, we make the following contributions: (i) We introduce the Gaussian variant of a popular tensor factorization approach for KG-enhanced recommendation. (ii) We propose Bayesian Critiquing with Indirect Evidence (BCIE), a knowledge-driven critiquing framework, and formulate a Bayesian closed-form user belief updating methodology to enable critiquing CRSs to address indirect feedback. (iii) We empirically show that BCIE results in considerable improvement of personalized recommendation over one-shot recommendation by evaluation on two datasets and a KG.\n\nBayesian Critiquing with Indirect Evidence\nIn the conversational critiquing problem setting that we investigate, …\nThe recommender’s duty in the next step is to update its belief in the user’s interests and refine 𝑅𝑢 given 𝑑𝑛, the evidence of the user’s taste observed from the critique at iteration 𝑛. Hence, the recommender needs a critique-modified recommendation function 𝑓𝑚, such that 𝑅ˆ 𝑢 = 𝑓𝑚 (𝑅𝑢, 𝑑𝑛). This process continues either for a fixed number of iterations or until the user accepts the recommendation or leaves the conversation.\nPre-critiquing phase\nWe build our recommender upon SimplE, a well-known tensor factorization-based KG embedding model, because of its efficient computations and full-expressiveness [8]. This model assigns two embedding vectors ℎ𝑒 and 𝑡𝑒 to each entity 𝑒 and two vectors 𝑣𝑟 and 𝑣𝑟 −1 to each relation 𝑟, and defines its scoring function for a triple (𝑒𝑖 , 𝑟, 𝑒𝑗) as Φ(𝑒𝑖 , 𝑟, 𝑒𝑗) = 1 2 (⟨ℎ𝑒,𝑖, 𝑣𝑟, 𝑡𝑒,𝑗⟩+⟨ℎ𝑒,𝑗, 𝑣−1 𝑟 , 𝑡𝑒,𝑖⟩), in which ⟨𝑣,𝑤, 𝑥⟩ = (𝑣 ⊙ 𝑤) · 𝑥 where ⊙ is element-wise and · represents dot product.\n\nSimplE Embedding for Link Prediction in Knowledge Graphs\n\n… Using the learned embeddings of entities and relations, the set of items yielding the highest plausibility scores for (𝑢𝑠𝑒𝑟,𝑙𝑖𝑘𝑒𝑠,𝑖𝑡𝑒𝑚) triples are picked for recommendation.\nCritiquing phase: Bayesian User Belief Updating with Indirect Evidence\nIn each critiquing session, the user provides knowledge-based feedback containing indirect evidence of her preference. … Hence, in the BCIE framework, we need to consider a distribution over representations of items that cover the user’s interest, which is denoted by 𝒛𝒎. To this end, we require to maintain a belief state over the user preferences, hereafter called user belief, which is initially centered at the learned embedding of the user entity and update it conditioned on the user critiques.\n\nThe next challenge is obtaining 𝑱𝒖,𝒛 . Note that by adopting the Gaussian variant of SimplE, the likelihood factor between the user belief distribution 𝒛𝑢 and item distribution 𝒛𝑚 becomes exp{−⟨𝒛𝑢,𝒓, 𝒛𝑚⟩} where 𝒓 is the embedding vector of the likes relation — this is log-bilinear in 𝒛𝑢 and 𝒛𝑚 and would appear to stymie closed-form Gaussian belief propagation. Serendipitously, we can rewrite ⟨𝒛𝒖,𝒓, 𝒛𝒎⟩ as 𝒖 𝑇 𝑫𝒓 𝒛, where 𝑫𝒓 is derived by reshaping 𝑟 as a diagonal matrix. Hence, we have 𝑱𝒖,𝒛 = 𝑫𝑟.\n\nTo summarize, while the use of a tensor-based likelihood introduced an unusual log-bilinear form and the need to marginalize over the latent item distribution induced by the KG critiques, we have shown that we can manipulate all necessary quantities in Gaussian form. In this way, we can perform a closed-form Gaussian user belief update w.r.t. an item distribution inferred by indirect KG properties.\nExperiments and Evaluation\nWe evaluate BCIE1 on two of the most popular recommendation datasets, MovieLens 20M 2 and Amazon-Book 3 , and acquire facts about their items from Freebase KG [2]. We consider ratings greater than 3.5 to indicate that the user likes an item and extract facts about items from Freebase using entity matching data from [20]. Also, since we conduct 5 steps of critiquing, we only keep items with at least 5 KG facts to enable selection of non-repetitive critiques. Table 1 shows dataset statistics.\nAs prior KG-enhanced recommendation studies have not considered the conversational setting and previous critiquing works do not handle knowledge-based indirect evidence, we propose two comparison baselines, namely ’Mapped items’ and ’Direct’. Mapped items is a heuristic method that maps each critique to a maximum of 10 relevant items from the KG and uses them for user belief updating. For example, for the critique “I prefer movies like Nolan’s works”, movies that are directed by, Nolan are mined from the KG and used as examples of the user’s interests.\n\n\n\n(from cedar.buffalo.edu/~srihari/CSE674/Chap7/7.1-MultiGauss.pdf)"},"RecSys/Bias-Reduction-News-Recommendation-System":{"slug":"RecSys/Bias-Reduction-News-Recommendation-System","filePath":"RecSys/Bias Reduction News Recommendation System.md","title":"Bias Reduction News Recommendation System","links":[],"tags":["recsys","todo"],"content":"Abstract\nNews recommender systems (NRS) are crucial for helping users navigate the vast amount of content available online. However, traditional NRS often suffer from biases that lead to a narrow and unfair distribution of exposure across news items. In this paper, we propose a novel approach, the Contextual-Dual Bias Reduction Recommendation System (C-DBRRS), which leverages Long Short-Term Memory (LSTM) networks optimized with a multi-objective function to balance accuracy and diversity. We conducted experiments on two real-world news recommendation datasets and the results indicate that our approach outperforms the baseline methods, and achieves higher accuracy while promoting a fair and balanced distribution of recommendations. This work contributes to the development of a fair and responsible recommendation system."},"RecSys/Consistent-Collaborative-Filtering-via-Tensor-Decomposition":{"slug":"RecSys/Consistent-Collaborative-Filtering-via-Tensor-Decomposition","filePath":"RecSys/Consistent Collaborative Filtering via Tensor Decomposition.md","title":"Consistent Collaborative Filtering via Tensor Decomposition","links":[],"tags":["recsys","todo"],"content":"Abstract\nCollaborative filtering is the de facto standard for analyzing users’ activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlinear mental models when evaluating items, allowing the existence of cycles in pairwise comparisons. We demonstrate the efficiency of SAD in both simulated and real world datasets containing over 1M user-item interactions. By comparing with seven SOTA collaborative filtering models with implicit feedbacks, SAD produces the most consistent personalized preferences, in the meanwhile maintaining top-level of accuracy in personalized recommendations. We release the model and inference algorithms in a Python library this https URL."},"RecSys/DCN-V2---Improved-Deep--and--Cross-Network-and-Practical-Lessons-for-Web-scale-Learning-to-Rank-Systems":{"slug":"RecSys/DCN-V2---Improved-Deep--and--Cross-Network-and-Practical-Lessons-for-Web-scale-Learning-to-Rank-Systems","filePath":"RecSys/DCN V2 - Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems.md","title":"DCN V2 - Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems","links":[],"tags":["recsys","todo"],"content":"Abstract\nLearning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep &amp; Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently.\nIn light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google."},"RecSys/DiffKG---Knowledge-Graph-Diffusion-Model-for-Recommendation":{"slug":"RecSys/DiffKG---Knowledge-Graph-Diffusion-Model-for-Recommendation","filePath":"RecSys/DiffKG - Knowledge Graph Diffusion Model for Recommendation.md","title":"DiffKG - Knowledge Graph Diffusion Model for Recommendation","links":[],"tags":["recsys"],"content":"Abstract\nKnowledge Graphs (KGs) have emerged as invaluable resources for enriching recommendation systems by providing a wealth of factual information and capturing semantic relationships among items. Leveraging KGs can significantly enhance recommendation performance. However, not all relations within a KG are equally relevant or beneficial for the target recommendation task. In fact, certain item-entity connections may introduce noise or lack informative value, thus potentially misleading our understanding of user preferences. To bridge this research gap, we propose a novel knowledge graph diffusion model for recommendation, referred to as DiffKG. Our framework integrates a generative diffusion model with a data augmentation paradigm, enabling robust knowledge graph representation learning. This integration facilitates a better alignment between knowledge-aware item semantics and collaborative relation modeling. Moreover, we introduce a collaborative knowledge graph convolution mechanism that incorporates collaborative signals reflecting user-item interaction patterns, guiding the knowledge graph diffusion process. We conduct extensive experiments on three publicly available datasets, consistently demonstrating the superiority of our DiffKG compared to various competitive baselines. We provide the source code repository of our proposed DiffKG model at the following link: : github.com/HKUDS/DiffKG.\nIntroduction\nThe recommendation performance in practical scenarios is significantly hindered by the inherent sparsity of user-item interactions [36, 40]. To mitigate this issue, the integration of a knowledge graph (KG) as a comprehensive information network for items has emerged as a new trend in collaborative filtering, known as knowledge-aware recommendation. Researchers have explored knowledge-aware recommendation through two primary approaches: embedding-based methods and path-based methods. … To combine the strengths of embedding-based and path-based methods, recent research has turned to GNNs as a powerful tool.\nDespite the demonstrated effectiveness of existing knowledge graph (KG)-aware recommendation methods, their performance heavily relies on high-quality input knowledge graphs and can be adversely affected by the presence of noise. … To address these challenges, recent research has proposed the utilization of contrastive learning (CL) techniques to enhance knowledge-aware recommendation.\n… , we propose a unique knowledge graph diffusion paradigm that effectively balances corruption and reconstruction.\n\nWe present a novel recommendation model called DiffKG, which leverages task-relevant item knowledge to enhance the collaborative filtering paradigm. Our approach introduces a new framework that allows for the distillation of high-quality signals from the aggregated representation of noisy knowledge graphs.\nWe propose an integration of the generative diffusion model with the knowledge graph learning framework, designed for knowledge-aware recommendation. This integration allows us to effectively align the semantics of knowledge-aware items with collaborative relation modeling for recommendation purposes.\nOur extensive experimental evaluations substantiate the substantial performance gains achieved by our DiffKG framework when compared to various baseline models across diverse benchmark datasets. Notably, our approach effectively tackles the challenges stemming from data noise and data scarcity, which are known to exert a negative impact on the accuracy of recommendation.\n\nThe Proposed DiffKG Framework\n\nContrastive learning has recently gained remarkable success in the realm of recommendation systems. … Unfortunately, the random augmentation can introduce unwanted noise, and the supplementary knowledge graph view may contain irrelevant information.\nTo tackle these challenges, we propose the use of a generative model to reconstruct a subgraph G ′ 𝑘 of the knowledge graph G𝑘 that specifically contains the relationships relevant to the downstream recommendation task.\nDiffusion with Knowledge Graph\n\nNoise Diffusion Process. In Fig. 2, we can observe that our knowledge graph (KG) diffusion, similar to other diffusion models, consists of two essential processes: the forward process and the reverse process. In order to apply these processes to the KG, we represent the KG using an adjacency matrix.\n… We initialize the initial state 𝝌0 as the original adjacency matrix z𝑖 of the item.\nKnowledge Graph Generation with Diffusion Model. In contrast to other diffusion models that randomly draw Gaussian noises for reverse generation, we have designed a simple inference strategy that aligns with the training of DiffKG for relation prediction in knowledge graphs (KGs).\nIn our inference strategy, we begin by corrupting the original KG relations 𝝌0 in a step-by-step manner during the forward process, resulting in 𝝌T ′ . We then set 𝝌ˆT = 𝝌T ′ and perform reverse denoising, where we ignore the variance and use 𝝌ˆ𝑡−1 = 𝜇𝜃 (𝝌ˆ𝑡 , 𝑡) for deterministic inference. … For each item 𝑖, we select the top 𝑘 zˆ 𝑗 𝑖 (𝑗 ∈ [0, |E | − 1], 𝑗 ∈ J, and |J | = 𝑘) and add 𝑘 relations between item 𝑖 and entities 𝑗 ∈ J.\nCollaborative Knowledge Graph Convolution. To mitigate the potential limitations of the diffusion model in generating a denoised knowledge graph that encompasses pertinent relationships for downstream recommendation tasks, we propose a collaborative knowledge graph convolution (CKGC) mechanism.\nThe loss of collaborative knowledge graph convolution, denoted as L𝑐𝑘𝑔𝑐 , is computed by incorporating user-item interaction information and knowledge graph predictions into the item embedding generation process. Specifically, we begin by aggregating the user-item interaction information A with the predicted relation probabilities from the knowledge graph, represented as 𝝌ˆ0. This aggregation updates the user-item interaction matrix, effectively integrating the knowledge graph information. Next, we combine this updated user-item matrix with the user embeddings E𝑢 to obtain an item embedding E ′ 𝑖 that jointly incorporates both the knowledge graph and user information. Finally, we calculate the mean squared error (MSE) loss between the aggregated item embedding E ′ 𝑖 and the original item embedding E𝑖 , and optimize it alongside the ELBO loss (L𝑒𝑙𝑏𝑜 ).\n\nExperiments\n\n\n"},"RecSys/Dynamic-Graph-Neural-Networks-for-Sequential-Recommendation":{"slug":"RecSys/Dynamic-Graph-Neural-Networks-for-Sequential-Recommendation","filePath":"RecSys/Dynamic Graph Neural Networks for Sequential Recommendation.md","title":"Dynamic Graph Neural Networks for Sequential Recommendation","links":[],"tags":["recsys"],"content":"Abstract\nModeling user preference from his historical sequences is one of the core problems of sequential recommendation. Existing methods in this field are widely distributed from conventional methods to deep learning methods. However, most of them only model users’ interests within their own sequences and ignore the dynamic collaborative signals among different user sequences, making it insufficient to explore users’ preferences. We take inspiration from dynamic graph neural networks to cope with this challenge, modeling the user sequence and dynamic collaborative signals into one framework. We propose a new method named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which connects different user sequences through a dynamic graph structure, exploring the interactive behavior of users and items with time and order information. Furthermore, we design a Dynamic Graph Recommendation Network to extract user’s preferences from the dynamic graph. Consequently, the next-item prediction task in sequential recommendation is converted into a link prediction between the user node and the item node in a dynamic graph. Extensive experiments on three public benchmarks show that DGSR outperforms several state-of-the-art methods. Further studies demonstrate the rationality and effectiveness of modeling user sequences through a dynamic graph.\nIntroduction\nAlthough these methods have achieved compelling results, we argue that these methods lack explicit modeling of the dynamic collaborative signals among different user sequences\n…(1) These models do not explicitly leverage the collaborative information among different user sequences, in other words, most of them focus on encoding each user’s own sequence, while ignoring the high-order connectivity between different user sequences,\n(2) These models ignore the dynamic influence of the high-order collaboration information at different times.\n\nfirstly, we convert all user sequences into a dynamic graph annotated with time and order information on edges (Section 4.1). Consequently, the user sequences having common items are associated with each other via user → item and item → user connections.\nSecondly, we devise a sub-graph sampling strategy (Section 4.2) to dynamically extract sub-graphs containing user’s sequence and associated sequences.\nThirdly, to encode user’s preference from the sub-graph, we design a Dynamic Graph Recommendation Network (DGRN) (Section 4.3), in which a dynamic attention module is constructed to capture the long-term preference of users and long-term character of items, and a recurrent neural module or attention module is further utilized to learn short-term preference and character of users and items, respectively.\nMethodology\nDynamic Graph Construction\nWhen the user u acts on the item i at time t, an edge e is established between u and i, and e can be represented by the quintuple (u, i, t, o^i_u, o^u_i)\no^i_u is the order of u−i interaction, that is, the position of item i in all items that the u has interacted with. o^u_i refers to the order of u in all user nodes that have interacted with item i.\n\nSub-Graph Sampling\nSpecifically, we first take user node u as the anchor node and select its most recent n first-order neighbors from graph G tk , that is, the historical items that u has interacted with, written as Nu, where n is the maximum length of user sequence (Line 5, 6, and 8 in Algorithm 1).\nNext, for each item i ∈ Nu, we use each of them as an anchor node to sample the set of users who have interacted with them, written as Ni (Line 11, 12, and 14 in Algorithm 1).\nFollowed by analogy, we can obtain the multi-hop neighbors of node u, which could forms u’s m-order sub-graph G m u (tk) of S u (m is hyper-parameter used to control the size of sub-graph).\nDynamic Graph Recommendation Networks\nSimilar to most GNNs, The DGRN component consists of message propagation and node updating components.\nThe message propagation mechanism aims to learn the message propagation information from user to item and item to user in G m u (tk), respectively. The challenge is how to encode the sequential information of neighbors from user and item perspectives, respectively.\nFrom item to user. … we need to extract two types of information from the neighbors of each user node, which are long-term preference and short-term preference respectively. The long-term preference [46] of user reflects his or her inherent characteristics and general preference, which can be induced from the user’s all historical items. The shortterm preference of the user reflects his or her latest interest.\nFrom user to item. … On the one hand, the long-term character can reflect the general characters of the item. For example, the wealthy people usually buy high-end cosmetics. On the other hand, short-term character reflects the newest property of item.\nLong-term Information.\n\nDynamic Graph Attention Mechanism.\n\nShort-term Information.\n\nNode updating\n\nRecommendation and Optimization\n\nExperiments\nTo evaluate the effectiveness of our model, we conduct experiments on three Amazon datasets from real-world platforms [48]: Amazon-CDs, Amazon-Games, and AmazonBeauty. These datasets are widely used in evaluating sequential recommendation methods and are varying in terms of domains, sizes, and sparsity.\n"},"RecSys/Explainable-Fairness-in-Recommendation":{"slug":"RecSys/Explainable-Fairness-in-Recommendation","filePath":"RecSys/Explainable Fairness in Recommendation.md","title":"Explainable Fairness in Recommendation","links":[],"tags":["recsys","todo"],"content":"Abstract\nExisting research on fairness-aware recommendation has mainly focused on the quantification of fairness and the development of fair recommendation models, neither of which studies a more substantial problem—identifying the underlying reason of model disparity in recommendation. This information is critical for recommender system designers to understand the intrinsic recommendation mechanism and provides insights on how to improve model fairness to decision makers. Fortunately, with the rapid development of Explainable AI, we can use model explainability to gain insights into model (un)fairness. In this paper, we study the problem of explainable fairness, which helps to gain insights about why a system is fair or unfair, and guides the design of fair recommender systems with a more informed and unified methodology. Particularly, we focus on a common setting with feature-aware recommendation and exposure unfairness, but the proposed explainable fairness framework is general and can be applied to other recommendation settings and fairness definitions. We propose a Counterfactual Explainable Fairness framework, called CEF, which generates explanations about model fairness that can improve the fairness without significantly hurting the performance. CEF framework formulates an optimization problem to learn the “minimal” change of the input features that changes the recommendation results to a certain level of fairness. Based on the counterfactual recommendation result of each feature, we calculate an explainability score in terms of the fairness-utility trade-off to rank all the feature-based explanations, and select the top ones as fairness explanations."},"RecSys/Fairness-among-New-Items-in-Cold-Start-Recommender-Systems":{"slug":"RecSys/Fairness-among-New-Items-in-Cold-Start-Recommender-Systems","filePath":"RecSys/Fairness among New Items in Cold Start Recommender Systems.md","title":"Fairness among New Items in Cold Start Recommender Systems","links":[],"tags":["recsys","todo"],"content":"Abstract\nThis paper investigates recommendation fairness among new items. While previous efforts have studied fairness in recommender systems and shown success in improving fairness, they mainly focus on scenarios where unfairness arises due to biased prior user-feedback history (like clicks or views). Yet, it is unknown whether new items without any feedback history can be recommended fairly, and if unfairness does exist, how can we provide fair recommendations among these new items in such a cold-start scenario. In detail, we first formalize fairness among new items with the well-known concepts of equal opportunity and Rawlsian Max-Min fairness. We empirically show the prevalence of unfairness in cold start recommender systems. Then we propose a novel learnable post-processing framework as a model blueprint for enhancing fairness, with which we propose two concrete models: a joint-learning generative model, and a score scaling model. Extensive experiments over four public datasets show the effectiveness of the proposed models for enhancing fairness while also preserving recommendation utility."},"RecSys/Frequency-Enhanced-Hybrid-Attention-Network-for-Sequential-Recommendation":{"slug":"RecSys/Frequency-Enhanced-Hybrid-Attention-Network-for-Sequential-Recommendation","filePath":"RecSys/Frequency Enhanced Hybrid Attention Network for Sequential Recommendation.md","title":"Frequency Enhanced Hybrid Attention Network for Sequential Recommendation","links":[],"tags":["recsys"],"content":"Abstract\nThe self-attention mechanism, which equips with a strong capability of modeling long-range dependencies, is one of the extensively used techniques in the sequential recommendation field. However, many recent studies represent that current self-attention based models are low-pass filters and are inadequate to capture high-frequency information. Furthermore, since the items in the user behaviors are intertwined with each other, these models are incomplete to distinguish the inherent periodicity obscured in the time domain. In this work, we shift the perspective to the frequency domain, and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation, namely FEARec. In this model, we firstly improve the original time domain self-attention in the frequency domain with a ramp structure to make both low-frequency and high-frequency information could be explicitly learned in our approach. Moreover, we additionally design a similar attention mechanism via auto-correlation in the frequency domain to capture the periodic characteristics and fuse the time and frequency level attention in a union model. Finally, both contrastive learning and frequency regularization are utilized to ensure that multiple views are aligned in both the time domain and frequency domain. Extensive experiments conducted on four widely used benchmark datasets demonstrate that the proposed model performs significantly better than the state-of-the-art approaches.\nIntroduction\nDespite their effectiveness, self-attention used in current Transformer based models is constantly a low-pass filter, which continuously erases high-frequency information according to the theoretical justification in [13, 14].\nTo alleviate these issues, existing methods import local constraints in different ways to complement Transformer-based models. Such as LSAN [17] adopts a novel twin-attention paradigm to capture the global and local preference signals via a self-attention branch and a convolution branch module, respectively.\nMoreover, users’ behaviors on the Internet tend to show certain periodic trends [20–22]. … However, it is difficult to find the periodic behavior patterns hidden in the sequence by directly calculating the overall attention scores of items in the time domain. But in the frequency domain, there emerges some methods [23] constructing models to recognize the periodic characterize with the help of the Fourier transform, which inspires us to tackle this challenge from a new perspective for recommendation.\n\nWe shift the perspective to the frequency domain and design a frequency ramp structure to improve existing time domain self-attention.\nWe propose a novel frequency domain attention based on an autocorrelation mechanism, which discovers similar period-based dependencies by aggregating most relative time delay sequences.\nWe unify the frequency ramp structure with vanilla self-attention and frequency domain attention in one framework and design a frequency domain loss to regularize the model training.\nWe conduct extensive experiments on four public datasets, and the experimental results imply the superiority of the FEARec compared to state-of-the-art baselines.\n\n\nProposed Method\nFrequency Enhanced Hybrid Attention Encoder\nBased on the embedding layer, we develop the item encoder by stacking 𝐿 Frequency Enhanced hybrid Attention (FEA) blocks, which generally consists of three modules, 𝑖.𝑒., frequency ramp structure, hybrid attention layer, and the point-wise Feed Forward Network (FFN).\nFrequency Ramp Structure. In FEARec, instead of preserving all frequency components, we only extract a subset of frequencies for each layer to guarantee that different attention blocks focus on different spectrums. This strategy is used in both time domain attention and frequency domain attention as shown in Figure 2.\n\n\nFrequency Domain Attention Layer. … As discussed in Section 1, by calculating the auto-correlation, we can find the most related time-delay sequences in the frequency domain and thus discover the periodicity hidden in the behaviors.\n\n\nMulti-Task Learning\nContrastive Learning. Contrastive learning aims to minimize the difference between differently augmented views of the same user and maximize the difference between the augmented sequences derived from different users.\nAlthough previous augmentations methods [8] including item cropping, masking, and reordering help to enhance the performance of SR models, the data-level augmentations cannot guarantee a high level of semantic similarity [7]. Instead of using typical data augmentations, we use a dropout-based augmentations methods as shown in the right part of Figure 2, which is proposed in [7, 49]. We let E𝑢 and E ′ 𝑢 pass through the FEA encoder twice for two output views H 𝐿 𝑢 and (H 𝐿 𝑢 ) ′ respectively and model the frequency components to construct harder positive samples by mixing the frequency feature extract from time domain self-attention and frequency autocorrelation attention.\nFrequency Domain Regularization. … Since time-domain and frequency-domain features represent the same semantics, but only in different domains, we assume that the frequency spectrum of similar time-domain features should also be similar. To ensure the alignment of the representation of different augmented views in the frequency domain, we suggest an L1 regularization in the frequency domain as a complement to FEARec, which contributes to enriching the regularization of the spectrum of the augmented views.\nExperiment\n\n"},"RecSys/Knowledge-Prompt-tuning-for-Sequential-Recommendation":{"slug":"RecSys/Knowledge-Prompt-tuning-for-Sequential-Recommendation","filePath":"RecSys/Knowledge Prompt-tuning for Sequential Recommendation.md","title":"Knowledge Prompt-tuning for Sequential Recommendation","links":[],"tags":["recsys"],"content":"Abstract\nPre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users’ fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (KP4SR). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tree mask, which restores the data structure in a mask matrix form, thus mitigating the noise problem. We evaluate KP4SR on three real-world datasets, and experimental results show that our approach outperforms state-of-the-art methods on multiple evaluation metrics. Specifically, compared with PLM-based methods, our method improves NDCG@5 and HR@5 by 40.65% and 36.42% on the books dataset, 11.17% and 11.47% on the music dataset, and 22.17% and 19.14% on the movies dataset, respectively. Our code is publicly available at the link: github.com/zhaijianyang/KP4SR.\nIntroduction\n… However, most of these methods only model the IDs of users and items, considering only the user’s sequential preferences, and cannot capture the user’s fine-grained preferences.\n\nA straightforward and simple approach is to describe domain knowledge using natural language text and then use the powerful reasoning ability of PLMs to improve recommendation performance, as shown in Figure 1. However, there are two challenges with this approach: 1) How to convert structured knowledge graphs into text sequences. 2) Converting into text sequences may destroy the original data structure and how to deal with the noise caused by irrelevant entities and relationships.\n\nWe propose KP4SR, which, to the best of our knowledge, is the first work that transforms knowledge graphs into knowledge prompts (KP) to improve SR performance.\nWe construct KP, which addresses the problems of semantic difference between structured knowledge data contained in the KG and the sequential text data used by PLMs and allows for easy utilization of high-order information from the KG.\nWe propose prompt denoising (PD), which mitigates knowledge noise by restoring the KG data structure in the form of a mask matrix.\nWe conduct extensive experiments on three datasets, and the results demonstrate the effectiveness of our method. In addition, ablation experiments show that transforming the SR task into an NLP task still follows the general pattern of NLP, which indicates the great research prospects and research value of PLMs in improving the performance of recommendation systems.\n\n\nProblem Definition\nBy sorting the interactions between users and items by timestamps, we can obtain the interaction sequence 𝑆𝑢 of user 𝑢, which can be represented as 𝑆𝑢 = {𝑣 𝑢 1 , 𝑣𝑢 2 , …, 𝑣𝑢 |𝑢| }, where |𝑢| denotes the length of the sequence, 𝑢 ∈ U and 𝑣 ∈ V. Our goal is to predict the next item 𝑣 𝑢 |𝑢|+1 that the user is likely to interact with.\nOur goal is to incorporate domain knowledge from KG into PLMs to mine users’ complex preferences. For instance, given a basic input sample: “Tom has watched Cast Away, Back to the Future, and is going to watch [mask].”, we need to input the relevant KG information, such as (Cast Away, film.genre, Adventure).\nMethodology\nPrompts Construction\nMasked personalized prompts. … Specifically, MPP can transform the recommendation task into a pre-training task, namely a cloze task, as shown in Figure 3. For a user 𝑢 and his/her interaction sequence {𝐴, 𝐵,𝐶, 𝐷, 𝐸, 𝐹 }, we can fill in the corresponding fields in the template to obtain: User u has previously watched {A, B, C, D, E}, and is going to watch [mask] next. Here, [mask] is the next item to be predicted, i.e., the target item 𝐹 .\n\nMPP can transform the recommendation task into a pre-training task, improving task performance when downstream task data is sparse.\nKnowledge prompts. Using KG as side information in recommendation systems can significantly improve their performance.\nFor a triple (ℎ, 𝑟, 𝑡), where ℎ represents the head entity, 𝑟 represents the relation, and 𝑡 represents the tail entity, we manually design a relation template for each relation 𝑟 ∈ R to express the semantics of the corresponding triple. For example, in Figure 3, we design a template for the relation film.genre: The genre of [X] is [Y]. Then for the triple (Cast Away, film.genre, Adventure), we replace [X] and [Y] with the head and tail entities, respectively, to obtain a basic triple prompt: “The genre of Cast Away is Adventure.”.\nFused Prompt. After obtaining MPP and KP, we directly concatenate them as the input of PLM. Specifically, MPP can be represented as: 𝑋𝑑 = {𝑥1, 𝑥2, …, [𝑚𝑎𝑠𝑘], …, 𝑥𝑚}, where 𝑥𝑖 is the 𝑖-th token of the text sequence, 𝑚 represents the length of the text in tokens, and [𝑚𝑎𝑠𝑘] represents the next item to be predicted.\nPrompt Denoising\nConverting KG to knowledge prompts can disrupt the original data structure and introduce a large amount of irrelevant and noisy knowledge.\nKnowledge tree construction. … The root node of the knowledge tree is the MPP, which contains multiple items that the user has interacted with. Therefore, the knowledge tree has multiple knowledge subtrees.\n… For example, in Figure1, the movie “Cast Away” can be represented by two triplets: (Cast Away, genre, Adventure) and (Cast Away, starred, Tom Hanks). We use 𝐴 to represent “Cast Away”, 𝐴1 to represent “Adventure”, and 𝐴2 to represent “Tom Hanks.” By introducing the relationship templates “The genre of [X] is [Y].” and “[X] starring [Y].”, we obtain two triplet prompts for 𝐴: “𝐴𝐴1: The genre of Cast Away is Adventure.” and “𝐴𝐴2: Cast Away starring Tom Hanks.”. 𝐴𝐴1 and 𝐴𝐴2 are 1-hop triplet prompts for 𝐴.\nKnowledge tree mask. … Triple prompts without logical and semantic relationships will generate much noise, and we should limit their mutual influence.\n\n\nTraining and Recommendation\nWe employ the T5 model architecture [28],which is an encoder-decoder-based pre-trained language model using mask prediction as the pre-training task. We construct personalized prompts with masks, transforming the recommendation task into a mask prediction task similar to the pre-training task of PLMs.\nExperiments\nWe conduct experiments on three public datasets: Amazon books [12], LFM-1b [11], and Movielens-10M [30]. These datasets record the interaction information between users and books, music, and movies.\nThe KG we used is from KB4Rec [45], which links the above three widely used datasets with the widespread knowledge base Freebase[1] to provide side information for recommendation systems.\n"},"RecSys/Learning-Vector-Quantized-Item-Representation-for-Transferable-Sequential-Recommenders":{"slug":"RecSys/Learning-Vector-Quantized-Item-Representation-for-Transferable-Sequential-Recommenders","filePath":"RecSys/Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders.md","title":"Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders","links":[],"tags":["recsys","todo"],"content":"Abstract\nRecently, the generality of natural language text has been leveraged to develop transferable recommender systems. The basic idea is to employ pre-trained language models~(PLM) to encode item text into item representations. Despite the promising transferability, the binding between item text and item representations might be too tight, leading to potential problems such as over-emphasizing the effect of text features and exaggerating the negative impact of domain gap. To address this issue, this paper proposes VQ-Rec, a novel approach to learning Vector-Quantized item representations for transferable sequential Recommenders. The main novelty of our approach lies in the new item representation scheme: it first maps item text into a vector of discrete indices (called item code), and then employs these indices to lookup the code embedding table for deriving item representations. Such a scheme can be denoted as “text → code → representation”. Based on this representation scheme, we further propose an enhanced contrastive pre-training approach, using semi-synthetic and mixed-domain code representations as hard negatives. Furthermore, we design a new cross-domain fine-tuning method based on a differentiable permutation-based network. Extensive experiments conducted on six public benchmarks demonstrate the effectiveness of the proposed approach, in both cross-domain and cross-platform settings. Code and pre-trained model are available at: this https URL."},"RecSys/Monolith---Real-Time-Recommendation-System-With-Collisionless-Embedding-Table":{"slug":"RecSys/Monolith---Real-Time-Recommendation-System-With-Collisionless-Embedding-Table","filePath":"RecSys/Monolith - Real Time Recommendation System With Collisionless Embedding Table.md","title":"Monolith - Real Time Recommendation System With Collisionless Embedding Table","links":[],"tags":["recsys","todo"],"content":"Abstract\nBuilding a scalable and real-time recommendation system is vital for many businesses driven by time-sensitive customer feedback, such as short-videos ranking or online ads. Despite the ubiquitous adoption of production-scale deep learning frameworks like TensorFlow or PyTorch, these general-purpose frameworks fall short of business demands in recommendation scenarios for various reasons: on one hand, tweaking systems based on static parameters and dense computations for recommendation with dynamic and sparse features is detrimental to model quality; on the other hand, such frameworks are designed with batch-training stage and serving stage completely separated, preventing the model from interacting with customer feedback in real-time. These issues led us to reexamine traditional approaches and explore radically different design choices. In this paper, we present Monolith, a system tailored for online training. Our design has been driven by observations of our application workloads and production environment that reflects a marked departure from other recommendations systems. Our contributions are manifold: first, we crafted a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint; second, we provide an production-ready online training architecture with high fault-tolerance; finally, we proved that system reliability could be traded-off for real-time learning. Monolith has successfully landed in the BytePlus Recommend product."},"RecSys/PromptMM---Multi-Modal-Knowledge-Distillation-for-Recommendation-with-Prompt-Tuning":{"slug":"RecSys/PromptMM---Multi-Modal-Knowledge-Distillation-for-Recommendation-with-Prompt-Tuning","filePath":"RecSys/PromptMM - Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning.md","title":"PromptMM - Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning","links":["RecSys/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning"],"tags":["recsys"],"content":"Abstract\nMultimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM’s superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.\nIntroduction\nI1: Overfitting &amp; Sparsity. Current multimedia recommenders excel by employing advanced encoders to handle high-dimensional features from pre-trained extractors (CLIP-ViT[36], BERT[5]). The auxiliary modalities alleviate data sparsity, but inevitably lead to increased consumption [52].\nI2: Noise &amp; Semantic Gap. As side information, multimedia content has inherent inaccuracies and redundancies when modeling user preference with collaborative relations.\nTo cope with the above issues, we propose the following solutions: I1: Developing a multi-modal KD (PromptMM) recommendation framework to free the inference recommender from the additional feature reduction parameters, by using KD for model compression. … The three types of KD respectively convey i) Pure knowledge through a modified KL divergence[24] based on BPR loss[40]; ii) Fine-grained modality-aware list-wise ranking knowledge; iii) Modality-aware embedding KD through SCE loss [18], an enhanced version of MSE.\nI2: Developing two modules to tackle issues ’Noise &amp; Semantic Gap’ based on the KD framework: i) Semantic bridging soft prompt-tuning is meant to reduce the impact of redundancy by prompting teacher to deliver student-task adaptive knowledge.  … ii) Modality-aware disentangled denoising list-wise ranking KD is to adjust the influence of inaccuracies in modality-aware user preference.\n\nIn this work, we propose a novel multi-modal KD framework PromptMM for multimedia recommendation, which can produce a lightweight yet effective student inference recommender with minimal online inference time and resource consumption.\nWe integrate prompt-tuning with multi-modal KD to bridge the semantic gap between modality content and collaborative signals. Additionally, by disentangling the modality-aware ranking logits, the impact of noise in multimedia data is adjusted.\nWe conduct experiments to evaluate our model performance on real-world datasets. The results demonstrate our PromptMM outperforms state-of-the-art baselines. The ablation studies and further analysis show the effectiveness of sub-modules.\n\nMethodology\nThe goal of multi-modal recommender systems is to learn a function that predicts the likelihood of a user adopting an item, given an interaction graph G with multi-modal context X.\nTeacher-Student in CF. For optimization, we employ offline distillation [11] which is a two-stage process, for flexibility concerns. In the first stage, only the teacher is trained, and in the second stage, the teacher remains fixed while only the student is trained.\n\n\nSoft Prompt-Tuning as Semantic Bridge. Drawing inspiration from parameter efficient finetuning (PEFT) [25, 26], we employ soft prompt-tuning[25] as the solution.\n\nThe Power of Scale for Parameter-Efficient Prompt Tuning\n\n\n𝜂(·) denotes the dimensionality reduction function (e.g., PCA) for multi-modal features.\nHaving obtained prompt p, we apply it to the feature reduction layer R (·) in teacher T (·) for enhancing the overfitting teacher, while simultaneously conducting student-task adaptive knowledge distillation through the frozen teacher. To be specific, we transform our prompt p into the modality-specific module, i.e., p → p 𝑚, which allows the prompt to capture modality-specific information.\n\nDuring teacher training, the prompt module P (·) undergoes gradient descent with teacher T (·), affecting the teacher’s inference process. During student training, we employ offline knowledge distillation[11], freezing the teacher’s parameters 𝜃T and updating the prompt module P (·) again according to the student’s recommended loss, which allows the prompt p to provide additional guidance to the feature reduction process and distill task-relevant knowledge from teacher T (·).\nTo comprehensively obtain the quality collaborative signal and modality-aware user preference from teacher T (·), we have designed three types of KD paradigms to convey knowledge from different perspectives: i) Ranking KD; ii) Denoised Modality-aware Ranking KD; and iii) Modality-aware Embedding KD.\nPure Ranking KD.\n\nDenoised Modality-aware Ranking Disentangled KD. Previously encoded multi-modal content f 𝑚 𝑢 ,f 𝑚 𝑖 in teacher T (·) contains noise and can affect the modality-aware user preferences modeling.\nFor a 𝐾 samples ranking list, the predicted logits can be denoted as ylist = [ 𝑦 + 1 ;𝑦 − 2 , 𝑦− 3 , …, 𝑦− 𝑘 , …, 𝑦− 𝐾 ], where 𝑦 + and 𝑦 − are the scores of the observed edge A+ and unobserved edge A−, respectively.\n\nModality-aware Embedding Distillation. In addition to the logit-based KD, we propose to enhance our PromptMM framework with embedding-level distillation.\n\nModel Joint Training of PromptMM. We train our recommender using a multi-task learning scheme to jointly optimize PromptMM with the following tasks: i) the main user-item interaction prediction task, represented by LBPR; ii) the pair-wise robust ranking KD L𝑃𝑎𝑖𝑟𝐾𝐷 ; iii) the modality-aware list-wise disentangled KD L𝐿𝑖𝑠𝑡𝐾𝐷 ; iv) modality-aware embedding KD L𝐸𝑚𝑏𝐾𝐷 . The overall loss function L is given as follows:\n\nEvaluation\n\n\nTiktok: www.biendata.xyz/competition/icmechallenge2019/\nElectronics: www.biendata.xyz/competition/icmechallenge2019/\n\n"},"RecSys/RecMind---Large-Language-Model-Powered-Agent-For-Recommendation":{"slug":"RecSys/RecMind---Large-Language-Model-Powered-Agent-For-Recommendation","filePath":"RecSys/RecMind - Large Language Model Powered Agent For Recommendation.md","title":"RecMind - Large Language Model Powered Agent For Recommendation","links":[],"tags":["recsys"],"content":"Abstract\nWhile the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model’s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind’s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.\nIntroduction\n… most existing RS methods have been designed for specific tasks and are inadequate in generalizing to unseen recommendation tasks (Fan et al., 2023).\nThe first key component is Planning which enables the agent to break complex recommendation tasks into manageable steps for efficient handling of complex situations. Each step of planning involves thought, action and observation (see Figure 1 for examples and Section 3 for details). The agent is also equipped with Memory consisting of Personalized Memory and World Knowledge, each accessible through specific tools. The Tools enhance the agent’s functionality on top of the LLM, such as retrieving relevant knowledge, or assisting with the reasoning process.\n… we propose a new planning algorithm Self Inspiring (SI). At each intermediate planning step, the agent “self-inspires” to consider all previously explored paths for the next planning. Unlike existing Chain-of-Thoughts (CoT) (Wei et al., 2022) and Tree-of-Thoughts (ToT) (Yao et al., 2023) which discards states (thoughts) in previously explored paths when generating a new state, SI retains all previous states from all history paths when generating new state.\n\nWe introduce RecMind, the first LLM-powered agent designed for general recommendation purposes, which operates without the need for finetuning for domain adaptation across datasets or tasks.\nWe incorporate a novel self-inspiring (SI) planning technique in RecMind. This technique integrates multiple reasoning paths and offers an empirical improvement over currently popular methods, such as CoT and ToT.\nWe evaluate the effectiveness and generalizability of RecMind across five recommendation tasks and two datasets. Extensive experiments and analyses demonstrate that RecMind outperforms state-of-the-art (SOTA) LLM-based baselines that do not involve any fine-tuning and achieves competitive performance with a fully pre-trained expert recommendation model such as P5 (Geng et al., 2022). In addition, SI outperforms CoT and ToT on general reasoning tasks, showing that the proposed the impact of SI is beyond recommendation tasks.\n\nArchitecture\n\n\nAt m-th path and step t, SI generates the next step of planning by considering all previous paths, i.e., s (m) t+1 ∼ pθ(st+1|z (1), …, z(m) ). After exploring n paths, the RecMind obtains the final result y ∼ Pθ(x, z(1), …, z(n) ).\n\nMemory Information stored in memory, including Personalized Memory and World Knowledge, enables the model to access knowledge beyond what is inherently present in the LLM’s parameters. Using the Amazon Reviews dataset as an illustrative example, Personalized Memory includes individualized user information, such as their reviews or ratings for a particular item. World Knowledge consists of two components: the first component is item metadata information, which also falls under the domain-specific knowledge category; the second component involves real-time information that can be accessed through Web search tool.\nTool Use\n\nDatabase Tool: This tool translates natural language questions into SQL queries.\nSearch Tool: This tool employs a search engine (e.g., Google) to access real-time information.\nText Summarization Tool: This tool helps summarize lengthy texts by invoking a text summarization model from the Hugging Face Hub.\n\nExperiments\nWe evaluate the performance of the RecMind agent in various recommendation scenarios, i.e., rating prediction, sequential recommendation, direct recommendation, explanation generation, review summarization.\n\nRating prediction is an essential task in recommendation systems that aims to predict the rating that a user would give to a particular item.\n\nIn the scenario of the direct recommendation, RecMind predicts the recommended items from a candidate set of 100 items, where only one candidate is positive.\nFor sequential recommendation, the agent takes the names of the user’s historically interacted items in order as input. Next, the agent is prompted to predict the title of the next item that the user might interact with.\n\nIn explanation generation, we assess the performance of RecMind in crafting textual explanations that justify a user’s interaction with a specific item.\n\n"},"RecSys/SimplE-Embedding-for-Link-Prediction-in-Knowledge-Graphs":{"slug":"RecSys/SimplE-Embedding-for-Link-Prediction-in-Knowledge-Graphs","filePath":"RecSys/SimplE Embedding for Link Prediction in Knowledge Graphs.md","title":"SimplE Embedding for Link Prediction in Knowledge Graphs","links":[],"tags":["recsys","todo"],"content":"Abstract\nKnowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE’s code is available on GitHub at github.com/Mehran-k/SimplE."},"RecSys/The-MovieLens-Beliefs-Dataset---Collecting-Pre-Choice-Data-for-Online-Recommender-Systems":{"slug":"RecSys/The-MovieLens-Beliefs-Dataset---Collecting-Pre-Choice-Data-for-Online-Recommender-Systems","filePath":"RecSys/The MovieLens Beliefs Dataset - Collecting Pre-Choice Data for Online Recommender Systems.md","title":"The MovieLens Beliefs Dataset - Collecting Pre-Choice Data for Online Recommender Systems","links":[],"tags":["recsys"],"content":"Abstract\nAn increasingly important aspect of designing recommender systems involves considering how recommendations will influence consumer choices. This paper addresses this issue by introducing a method for collecting user beliefs about un-experienced goods – a critical predictor of choice behavior. We implemented this method on the MovieLens platform, resulting in a rich dataset that combines user ratings, beliefs, and observed recommendations. We document challenges to such data collection, including selection bias in response and limited coverage of the product space. This unique resource empowers researchers to delve deeper into user behavior and analyze user choices absent recommendations, measure the effectiveness of recommendations, and prototype algorithms that leverage user belief data, ultimately leading to more impactful recommender systems. The dataset can be found at grouplens.org/datasets/movielens/ml_belief_2024/.\nIntroduction\nPrevious research has shown that one useful type of data for understanding user choice and the mechanisms through which recommendations act is user belief data — the opinions that users have about goods they have not consumed.\nOur Contributions: In this paper we provide both a procedure to effectively collect belief data and an open-source dataset generated by implementing the procedure on the MovieLens platform for over a year. We rely on a simple economic model of decision-making in the context of recommender systems to guide the type of variation that is generated by our procedure.\nProcedure\nEach time the consumer enters the platform, they receive a set of recommendations, denoted 𝑟𝑖,𝑡. For now, we remain agnostic to how this set is generated. We consider that the recommendation directly shifts the user’s beliefs so that the user’s expected utility following the set of recommendations is given by E𝑝𝑖 [𝑢𝑖(𝑥𝑖,𝑛) | 𝑟𝑖,𝑡].\n\nwe want to design the procedure so that we have the necessary variation to (1) characterize the set of prior beliefs over the space of goods, (2) predict how beliefs map to consumption, and (3) identify how recommendations shift beliefs.\nChoosing the Set of Goods\nThe exact size of 𝑀𝑡 is calibrated by a parameter 𝑦. The criteria for inclusion are the following:\n\nAll-time popularity: the 50𝑦 most popular movies within the different genres 𝐺. Popularity is determined by the number of ratings. For each genre 𝑔, the ceil(𝑠𝑔 · 50𝑦) movies with the most ratings within the genre.\nRating: the 25𝑦 movies with the highest rating score within the different genres 𝐺. 3 For each genre 𝑔, select the ceil(𝑠𝑔 · 25𝑦) highest rating score within the genre.\nPopular recent releases: the 10𝑦 most popular, recently released movies within the different genres𝐺. For each genre 𝑔, select the ceil(𝑠𝑔·10𝑦) most rated movies that were released within the genre.\nTrendy releases: the 10𝑦 ‘trendiest’ movies within the different genres 𝐺. For each genre 𝑔, select ceil(𝑠𝑔 · 10𝑦) movies with the highest trendy score within the genre.\nSerendipity: 5𝑦 movies uniformly sampled within the different genres 𝐺. For each genre 𝑔, select ceil(𝑠𝑔 · 5𝑦) sampled uniformly at random within the genre.\n\nThis leads to having 𝑀𝑡 with approximately 100𝑦 movies.\nChoosing Elicited Goods\nAt any given period, we present 8 movies to user 𝑖 to elicit beliefs about, 𝐵 𝑡 𝑖 , chosen from 𝑀𝑡 . This appears as a row on the platform’s homepage. If a user refreshes the page, it generates a new set 𝐵 𝑡 𝑖 , replacing the movies for which beliefs were already elicited.\nWe choose the set of movies to elicit beliefs about for a user at a given time period 𝑡, 𝐵 𝑡 𝑖 , according to the following principles:\n\nBroad Sampling: 3 movies from 𝑀𝑡 𝑖 uniformly at random.\nElicitation with possible recommendation: 4 movies from 𝑅 𝑡 𝑖 sampled uniformly at random from the top 𝑛 according to their top picks (where 𝑛 = 100). This serves the goal of generating variation in the set of elicited movies that may also possibly be recommended, while also sampling from more niche movies due to individual-level heterogeneity in user recommendations.\nSample New Movies: 1 movie from 𝑀𝑡 𝑖 that is a recent release.\n\n\nData\nData Collection Details … The only change is that our intervention changes the second row of the homepage of the platform to ask users about the movies chosen by the procedure.\nThe data collection began at the start of March 2023 and concluded in May 2024 . We began the data collection by setting 𝑦 = 100 for 𝑀𝑡 , but revised this down to 𝑦 = 11 in July 2023 once we had a better sense of users’ response rates.\nDatasets … The dataset contains the following fields, beyond the timestamp and user/movie identifiers:\n\nisSeen: This is −1 if the user did not respond, 0 if the user marked that they had not seen the movie, and 1 if the user marked that they had seen the movie.\nuserElicitRating / watchDate: This is the rating the user gave for the movie and the time they claim to have seen it.\nuserPredictRating / userCertainty: This is the predicted rating and certainty level the user has conditional on them not having seen the movie.\n\nIn addition, we provide ratings data that has a similar structure to the main MovieLens ratings datasets that have historically been released by GroupLens, except that the set of users are those that provided at least one belief data point."},"RecSys/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning":{"slug":"RecSys/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning","filePath":"RecSys/The Power of Scale for Parameter-Efficient Prompt Tuning.md","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","links":[],"tags":["recsys","todo"],"content":"Abstract\nIn this work, we explore “prompt tuning”, a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s “few-shot” learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning."},"RecSys/Towards-Empathetic-Conversational-Recommender-Systems":{"slug":"RecSys/Towards-Empathetic-Conversational-Recommender-Systems","filePath":"RecSys/Towards Empathetic Conversational Recommender Systems.md","title":"Towards Empathetic Conversational Recommender Systems","links":[],"tags":["recsys"],"content":"Abstract\nConversational recommender systems (CRSs) are able to elicit user preferences through multi-turn dialogues. They typically incorporate external knowledge and pre-trained language models to capture the dialogue context. Most CRS approaches, trained on benchmark datasets, assume that the standard items and responses in these benchmarks are optimal. However, they overlook that users may express negative emotions with the standard items and may not feel emotionally engaged by the standard responses. This issue leads to a tendency to replicate the logic of recommenders in the dataset instead of aligning with user needs. To remedy this misalignment, we introduce empathy within a CRS. With empathy we refer to a system’s ability to capture and express emotions. We propose an empathetic conversational recommender (ECR) framework.\nECR contains two main modules: emotion-aware item recommendation and emotion-aligned response generation. Specifically, we employ user emotions to refine user preference modeling for accurate recommendations. To generate human-like emotional responses, ECR applies retrieval-augmented prompts to fine-tune a pre-trained language model aligning with emotions and mitigating hallucination. To address the challenge of insufficient supervision labels, we enlarge our empathetic data using emotion labels annotated by large language models and emotional reviews collected from external resources. We propose novel evaluation metrics to capture user satisfaction in real-world CRS scenarios. Our experiments on the ReDial dataset validate the efficacy of our framework in enhancing recommendation accuracy and improving user satisfaction.\nIntroduction\nA crucial aspect of CRSs is to elicit user preferences through multi-turn dialogues, with two main subtasks: item recommendation and response generation [9]. A prominent challenge is the lack of sufficient contextual information for accurately modeling user preferences.\n… leads to a tendency of CRS to replicate the logic of recommenders in the dataset instead of addressing user needs.\n\nTheir work suggests that capturing emotions expressed in user utterances within dialogues is prominent for achieving accurate user preference modeling for item recommendation\nTo construct empathetic CRSs, we face two major challenges: (i) how to accurately model user preferences using emotions; and (ii) how to generate emotional responses contributing to user satisfaction. To address these challenges, we propose an empathetic conversational recommender (ECR) framework comprising two key modules: emotion-aware item recommendation and emotion-aligned response generation.\nThe contributions of this paper are as follows:\n\nTo bridge the gap between system outputs and user needs, we define empathy within a CRS and propose a novel framework ECR.\nWe augment user preference modeling by integrating their emotions, with a new training strategy to minimize the impact of incorrect labels.\nWe fine-tune a PLM to express emotions and apply retrieval-augmented prompts to mitigate hallucination.\nWe use LLMs to annotate user emotions and collect emotional reviews from external resources as empathetic CRS training data, which facilitates future research in this area.\nWe propose new evaluation metrics tailored to user satisfaction in real-world CRS scenarios, and our experimental results demonstrate that ECR significantly outperforms baselines on the ReDial dataset.\n\nMethod\n\nUser Emotion Extraction. … we employ GPT-3.5-turbo [45] to initially annotate user emotions in 5,082 utterances from the ReDial dataset. We limit the number of annotated emotions per utterance to a maximum of two labels. In annotating with GPT-3.5-turbo [45] for utterance-level user emotions, we adopted nine emotion labels: “like,” “curious,” “happy,” “grateful,” “negative,” “neutral,” “nostalgia,” “agreement,” and “surprise.” The “negative” label, denoting adverse emotions, accounted for 8.0%.\n… Based on the annotations, we fine-tune a GPT-2 model, which achieves 87.75% in terms of Recall@2 in categorizing emotions.\nEmotional Response Construction. … To construct a emotional review database 𝑅, we collect movie reviews from IMDb. … Each emotional review sentence 𝑟 = {𝑤𝑗 } |𝑟| 𝑗=1 consists of a sequence of words. From each emotional review sentence 𝑟, we extract a list of entities 𝐸𝑟 = {𝑒𝑗 } |𝐸𝑟 | 𝑗=1 , 𝑒𝑗 ∈ E. We then retrieve a set of knowledge triples T𝑟 = {⟨𝑖𝑟,𝑙, 𝑒𝑗⟩, 𝑒𝑗 ∈ 𝐸𝑟 } | T𝑟 | 𝑗=1 from the knowledge graph G, where the item 𝑖𝑟 is the head entity, the entity 𝑒𝑗 ∈ 𝐸𝑟 is the tail entity.\nLocal emotion-aware entity representing. we characterize utterance-level user emotions F𝑢 𝑢 𝑘 = {𝑓𝑗 } | F𝑢 𝑢 𝑘 | 𝑗=1 as reflecting emotions towards entities mentioned by the user in the current utterance …\n\nGlobal emotion-aware entity representing. We first use utterance-level user emotions to filter global entities and then aggregate their representations. Concretely, we assume that if a user exhibits similar emotions towards both 𝑒𝑗 and 𝑒𝑖 in a conversation, then 𝑒𝑖 is globally emotion-related to 𝑒𝑗 .\n\nFeedback-aware Item Reweighting. … we introduce a mapping function 𝑚(𝑓𝑖𝑘 ) that converts each user feedback 𝑓𝑖𝑘 as a weight scalar. The mapping function converts negative or unclear feedback into a lower weight.\n\nEmotion-aligned Generator. … Specifically, during the training stage, given the extracted knowledge entities 𝐸𝑟 and the retrieved knowledge triples T𝑟, we transform the entities and triples into word sequences, represented as 𝑆T𝑟 and 𝑆𝐸𝑟 . The prompt for generating emotional review 𝑟 consists of the word sequence of the knowledge entities 𝑆𝐸𝑟 , knowledge triples 𝑆T𝑟 , and the item name 𝑆𝑖𝑟 . Then, we incorporate the recommendation response 𝑢 𝑟 𝑡 into the prompt, guiding the model to generate contextually relevant responses.\n\nExperiments\nObjective evaluation metrics. … To validate the model’s effectiveness in estimating user preferences while negating the logged errors in the dataset, we calculate Recall_True@𝑛 (RT@𝑛, where 𝑛 = 1, 10, 50). This metric refines Recall@𝑛 but only considers the items that get the user feedback of “like” as the standard answers.\nSubjective evaluation metrics. … Following Wang et al. [33], we employ an LLM-based scorer capable of automatically assigning scores based on specific prompts to alleviate the evaluation reliance on human annotations and randomly sampling 1,000 examples for evaluation. In this context, GPT4-turbo from the OpenAI serves as the scoring tool. Given the inherent instability in LLMs, we invite three human annotators to assess the reliability of our LLM-based scorer’s evaluation results. The annotators are enlisted to rate 200 examples.\n\n"},"RecSys/Towards-Hierarchical-Policy-Learning-for-Conversational-Recommendation-with-Hypergraph-based-Reinforcement-Learning":{"slug":"RecSys/Towards-Hierarchical-Policy-Learning-for-Conversational-Recommendation-with-Hypergraph-based-Reinforcement-Learning","filePath":"RecSys/Towards Hierarchical Policy Learning for Conversational Recommendation with Hypergraph-based Reinforcement Learning.md","title":"Towards Hierarchical Policy Learning for Conversational Recommendation with Hypergraph-based Reinforcement Learning","links":[],"tags":["recsys"],"content":"Abstract\nConversational recommendation systems (CRS) aim to timely and proactively acquire user dynamic preferred attributes through conversations for item recommendation. In each turn of CRS, there naturally have two decision-making processes with different roles that influence each other: 1) director, which is to select the follow-up option (i.e., ask or recommend) that is more effective for reducing the action space and acquiring user preferences; and 2) actor, which is to accordingly choose primitive actions (i.e., asked attribute or recommended item) that satisfy user preferences and give feedback to estimate the effectiveness of the director’s option. However, existing methods heavily rely on a unified decision-making module or heuristic rules, while neglecting to distinguish the roles of different decision procedures, as well as the mutual influences between them. To address this, we propose a novel Director-Actor Hierarchical Conversational Recommender (DAHCR), where the director selects the most effective option, followed by the actor accordingly choosing primitive actions that satisfy user preferences. Specifically, we develop a dynamic hypergraph to model user preferences and introduce an intrinsic motivation to train from weak supervision over the director. Finally, to alleviate the bad effect of model bias on the mutual influence between the director and actor, we model the director’s option by sampling from a categorical distribution. Extensive experiments demonstrate that DAHCR outperforms state-of-the-art methods.\nIntroduction\n\nConversational recommendation systems (CRS) aim to dynamically learn user preferences by iteratively interacting with the user.\nFor each turn in CRS, the system naturally includes two essential decision-make procedures, when to recommend (i.e., ask or recommend), and what to talk about (i.e., the specific attribute/items). … Early works [Lei et al., 2020a; Sun and Zhang, 2018] develop policy learning for a subset of decision procedures and outsource the other procedures to heuristic rules (SCPR as illustrated in Figure 1 (a)). These works isolate strategies for different decisions and make policy learning hard to converge due to their lack of mutual influence during training. To solve this problem, Deng et al. [2021] and Zhang et al. [2022] develop unified policy learning frameworks (Unicorn as illustrated in Figure 1 (b))\n… Despite effectiveness, the unified strategy brings out issues to be solved: (i) The unified strategy complicates the action selection of the CRS strategy by enlarging the action space and introducing data bias into the action space due to the imbalance in the number of items and attributes. (ii) The unified strategy ignores the different roles of the two decision procedures, leading to the sub-optimal CRS strategy.\nThere remain three challenges in modeling these two roles and their mutual influence. The first challenge is weak supervision. … The second challenge is user preference modeling. In the scenario of CRS, the user likes/dislikes items since they satisfy some attributes, which is a three-order relation (i.e., user-attribute-item). To specify the attributes that motivate the user to like/dislike the item, we should model user preferences with such high-order relations. The third challenge is the bad effect of model bias [Battaglia et al., 2018; Tarvainen and Valpola, 2017] on the mutual influence between director and actor.\n\nWe emphasize the different roles in two decision procedures for CRS, and the mutual influence between them.\nWe propose a novel Director-Actor Hierarchical conversational recommender with intrinsic motivation to train from weak supervision and a dynamic hypergraph to learn user preferences from high-order relations. To alleviate the bad effect of model bias on the mutual influence between director and actor, DAHCR models the director’s options by sampling from a categorical distribution with Gumbel-softmax.\nWe conduct extensive experiments on two benchmark datasets, and DAHCR effectively improves the performance of conversational recommendation.\n\nThe Proposed Model\nThen at each turn t, MCR can either ask the user an attribute pt ∈ Pcand or recommend a certain number of items (e.g., the top ten items) Vt ⊆ Vcand to the user. According to the target item v ∗ and its associated attributes Pv ∗ , the user will choose to accept or reject the proposal of MCR. Based on the user’s feedback, MCR will update the candidate attribute set Pcand and the candidate item set Vcand.\nDAHCR Framework\n\n\n\n\nDAHCR Policy Learning\n\n\nExperiments\n\n"},"RecSys/Unlocking-the-Hidden-Treasures---Enhancing-Recommendations-with-Unlabeled-Data":{"slug":"RecSys/Unlocking-the-Hidden-Treasures---Enhancing-Recommendations-with-Unlabeled-Data","filePath":"RecSys/Unlocking the Hidden Treasures - Enhancing Recommendations with Unlabeled Data.md","title":"Unlocking the Hidden Treasures - Enhancing Recommendations with Unlabeled Data","links":[],"tags":["recsys","todo"],"content":"Abstract\nCollaborative filtering (CF) stands as a cornerstone in recommender systems, yet effectively leveraging the massive unlabeled data presents a significant challenge. Current research focuses on addressing the challenge of unlabeled data by extracting a subset that closely approximates negative samples. Regrettably, the remaining data are overlooked, failing to fully integrate this valuable information into the construction of user preferences. To address this gap, we introduce a novel positive-neutral-negative (PNN) learning paradigm. PNN introduces a neutral class, encompassing intricate items that are challenging to categorize directly as positive or negative samples. By training a model based on this triple-wise partial ranking, PNN offers a promising solution to learning complex user preferences. Through theoretical analysis, we connect PNN to one-way partial AUC (OPAUC) to validate its efficacy. Implementing the PNN paradigm is, however, technically challenging because: (1) it is difficult to classify unlabeled data into neutral or negative in the absence of supervised signals; (2) there does not exist any loss function that can handle set-level triple-wise ranking relationships. To address these challenges, we propose a semi-supervised learning method coupled with a user-aware attention model for knowledge acquisition and classification refinement. Additionally, a novel loss function with a two-step centroid ranking approach enables handling set-level rankings. Extensive experiments on four real-world datasets demonstrate that, when combined with PNN, a wide range of representative CF models can consistently and significantly boost their performance. Even with a simple matrix factorization, PNN can achieve comparable performance to sophisticated graph neutral networks. Our code is publicly available at github.com/Asa9aoTK/PNN-RecBole.\nIntroduction\nMost existing works [2, 3, 5, 18, 36] make an intuitive assumption that unlabeled data can directly provide negative signals. Yet, recent works reveal a nuanced reality: unlabeled data and negative samples harbor an inevitable disparity.\nIn this paper, we pose a fundamental question: Is it feasible to fully unearth and leverage the information within massive unlabeled data? An intuitive approach might be to initially follow the existing methods by filtering out some negative samples and treating the remaining items as positive instances. … We cannot guarantee that the remaining items exclusively belong to the positive class.\nOur answer is a novel generic positive-neutral-negative (PNN) learning paradigm. … With this new class, our goal is to train a model based on partial rankings Σ 𝑢 𝑝𝑜𝑠 &gt;𝑢 Σ 𝑢 𝑛𝑒𝑢 &gt;𝑢 Σ 𝑢 𝑛𝑒𝑔 (i.e., 𝑢 prefers items Σ 𝑢 𝑝𝑜𝑠 over Σ 𝑢 𝑛𝑒𝑢 over Σ 𝑢 𝑛𝑒𝑔).\nWhile the conceptual framework of the PNN paradigm effectively addresses the aforementioned issue, providing a concrete implementation is technically challenging for at least two reasons:\n\nSparse supervised signals. Reliable signals primarily stem from user interactions, which tend to be sparse compared to the pool of unlabeled data. This scarcity impedes the effective identification of neutral and negative items within massive unlabeled data.\nConstrained positive-negative loss functions. Traditional loss functions categorize all items into either positive or negative classes, failing to accommodate the nuanced nature of neutral items. Even if these latent neutral items are unearthed, they are inevitably pigeonholed into positive or negative classifications.\n\nWe summarize our main contributions as follows:\n\nWe introduce the novel PNN learning paradigm, a pioneering approach to fully exploit the wealth of information within massive unlabeled data in CF. By incorporating a third neutral class and leveraging set-level ranking relationships, PNN addresses the complexity of unlabeled data. Furthermore, through mathematical analysis, we establish the relationship between PNN and OPAUC, demonstrating PNN’s ability to optimize various indicators of the recommendation system.\nWe propose a concrete implementation of the PNN learning paradigm which can be seamlessly integrated with multiple mainstream CF models. It features a semi-supervised learning method with a user-aware attention model to reliably classify unlabeled data and a two-step centroid ranking approach to accommodate set-level rankings.\nWe perform extensive experiments on four public datasets to demonstrate that, when combined with PNN, a wide variety of mainstream CF models can consistently and substantially boost their performance, confirming the value of PNN.\n\nPNN Meets OPAUC\nOPAUC = one-way partial AUC (OPAUC)\nWe are integrating the ROC but just between [\\gamma, \\delta]\n\n. It becomes evident that neutral samples and the set-level rankings introduced by PNN can directly optimize the OPAUC metric, thereby enhancing various evaluation metrics of recommender systems.\nMethodology\n… Given user 𝑢 ∈ 𝑈 , we formulate the loss function as follows:\nL𝑢 = (1 − 𝜆)LBPR + 𝜆L\n… Similarly, how to design 𝜆 is also a challenging task due to a lack of supervised signals. Inspired by previous works [6, 15], we propose a user-aware attention model to indirectly assess the classification performance based on the user’s positive items sets Σ 𝑢 𝑝𝑜𝑠 . Intuitively, if the model can correctly identify these items as positive samples, it suggests that the model has adequately learned user preferences from the BPR loss. Following this intuition, we devise 𝜆 as follows:\n\n\nWhen our model possesses sufficient knowledge, the positive items should exhibit a high degree of similarity to the user’s preferences. As a result, 𝛼 𝑎𝑡𝑡𝑟 𝑖 will receive a higher value. Consequently, the weight 𝜆 will also increase, prioritizing the significance of LPNN in the overall loss function.\n\nUniform Loss … Since the BPR loss is designed to set positive items apart from unlabeled data , it may lead to clusters of unlabeled data with similar scores, which is detrimental to our classification goal.\n\nClamp Embeddings … Inspired by previous works [7, 9, 33, 35], in the second step, we propose a novel clamp mechanism to adaptively generate the margin. Our insight is to utilize two clamp embeddings to tightly constrain the neutral classes in the embedding space.\n\n\nExperiments\n\n"},"Robotics/DexterityGen---Foundation-Controller-for-Unprecedented-Dexterity":{"slug":"Robotics/DexterityGen---Foundation-Controller-for-Unprecedented-Dexterity","filePath":"Robotics/DexterityGen - Foundation Controller for Unprecedented Dexterity.md","title":"DexterityGen - Foundation Controller for Unprecedented Dexterity","links":[],"tags":["robotics","todo"],"content":"Abstract\nTeaching robots dexterous manipulation skills, such as tool use, presents a significant challenge. Current approaches can be broadly categorized into two strategies: human teleoperation (for imitation learning) and sim-to-real reinforcement learning. The first approach is difficult as it is hard for humans to produce safe and dexterous motions on a different embodiment without touch feedback. The second RL-based approach struggles with the domain gap and involves highly task-specific reward engineering on complex tasks. Our key insight is that RL is effective at learning low-level motion primitives, while humans excel at providing coarse motion commands for complex, long-horizon tasks. Therefore, the optimal solution might be a combination of both approaches. In this paper, we introduce DexterityGen (DexGen), which uses RL to pretrain large-scale dexterous motion primitives, such as in-hand rotation or translation. We then leverage this learned dataset to train a dexterous foundational controller. In the real world, we use human teleoperation as a prompt to the controller to produce highly dexterous behavior. We evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that it is a general-purpose controller that can realize input dexterous manipulation commands and significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks. Notably, with DexGen we demonstrate unprecedented dexterous skills including diverse object reorientation and dexterous tool use such as pen, syringe, and screwdriver for the first time."},"Robotics/HAMSTER---Hierarchical-Action-Models-For-Open-World-Robot-Manipulation":{"slug":"Robotics/HAMSTER---Hierarchical-Action-Models-For-Open-World-Robot-Manipulation","filePath":"Robotics/HAMSTER - Hierarchical Action Models For Open-World Robot Manipulation.md","title":"HAMSTER - Hierarchical Action Models For Open-World Robot Manipulation","links":[],"tags":["robotics","todo"],"content":"Abstract\nLarge foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy’s burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: this https URL"},"Robotics/HOVER---Versatile-Neural-Whole-Body-Controller-for-Humanoid-Robots":{"slug":"Robotics/HOVER---Versatile-Neural-Whole-Body-Controller-for-Humanoid-Robots","filePath":"Robotics/HOVER - Versatile Neural Whole-Body Controller for Humanoid Robots.md","title":"HOVER - Versatile Neural Whole-Body Controller for Humanoid Robots","links":[],"tags":["robotics","todo"],"content":"Abstract\nHumanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications."},"Robotics/Learning-Human-to-Humanoid-Real-Time-Whole-Body-Teleoperation":{"slug":"Robotics/Learning-Human-to-Humanoid-Real-Time-Whole-Body-Teleoperation","filePath":"Robotics/Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation.md","title":"Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation","links":[],"tags":["robotics","todo"],"content":"Abstract\nWe present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable “sim-to-data” process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation."},"Robotics/Magma---A-Foundation-Model-for-Multimodal-AI-Agents":{"slug":"Robotics/Magma---A-Foundation-Model-for-Multimodal-AI-Agents","filePath":"Robotics/Magma - A Foundation Model for Multimodal AI Agents.md","title":"Magma - A Foundation Model for Multimodal AI Agents","links":[],"tags":["robotics","todo"],"content":"Abstract\nWe present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at this https URL."},"Robotics/π_{0.5}---a-Vision-Language-Action-Model-with-Open-World-Generalization":{"slug":"Robotics/π_{0.5}---a-Vision-Language-Action-Model-with-Open-World-Generalization","filePath":"Robotics/π_{0.5} - a Vision-Language-Action Model with Open-World Generalization.md","title":"π_{0.5} - a Vision-Language-Action Model with Open-World Generalization","links":[],"tags":["robotics","todo"],"content":"Abstract\nIn order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe \\pi_{0.5}, a new model based on \\pi_{0} that uses co-training on heterogeneous tasks to enable broad generalization. \\pi_{0.5}\\ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes."},"index":{"slug":"index","filePath":"index.md","title":"👋 Welcome","links":["RecSys/","Robotics/","Causal/"],"tags":[],"content":"This is where I put whatever I’m currently reading or interested in — mostly papers, notes, and personal reflections.\n📚 Topics\n\nRecommender Systems\n→ See: RecSys\n\n\nRobotics\n→ See: Robotics\nCausal Inference\n→ See: Causal\n\nFeel free to explore via the graph view (top right), tags, or backlinks. Everything is a work in progress — and that’s the point."}}